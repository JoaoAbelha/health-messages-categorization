{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23919,"status":"ok","timestamp":1655712016543,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"BR8Mgz8aE0zC","outputId":"f9dff8ff-3f5f-4421-aca2-31ded988c87e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":454,"status":"ok","timestamp":1655712016985,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"1JiopdCUVutc"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1655712016986,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"MpI-t6wsE45B"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('/content/gdrive/My Drive/Mestrado//second_level_preprocessed.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1655712016987,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"jhVK5M7nFGLX"},"outputs":[],"source":["from collections import defaultdict\n","result = defaultdict(int)\n","for labels in df.labels:\n","    result[len(labels)] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1655712016989,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"R7TfMtxZaG4N","outputId":"95bbf998-7cd2-4091-9a19-5b4f521ecda6"},"outputs":[],"source":["result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1655712017372,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"cIk1hkgSYi8e","outputId":"7a25a220-d396-4916-fa58-c0360d5bfe5f"},"outputs":[],"source":["names = list(result.keys())\n","import matplotlib.pyplot as plt\n","\n","values = list(result.values())\n","\n","plt.bar(names, values)\n","plt.xticks(rotation=90)\n","plt.ylabel('frequency')\n","plt.xlabel('number of labels / Document')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"elapsed":542,"status":"error","timestamp":1655712017896,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"thlEOqbaPCFT","outputId":"43a948cb-1cf4-44f3-f658-620f97cde07a"},"outputs":[],"source":["result.plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":519,"status":"aborted","timestamp":1655712017875,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"NdRuKg6vE68B"},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":524,"status":"aborted","timestamp":1655712017880,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"2NLv5uznBn9A"},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":525,"status":"aborted","timestamp":1655712017881,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"ogyqjcwYFCVR"},"outputs":[],"source":["import string\n","regular_punct = list(string.punctuation)\n","def remove_punctuation(text,punct_list):\n","    for punc in punct_list:\n","        if punc in text:\n","            text = text.replace(punc, ' ' )\n","    return text.strip()\n","\n","df['punct'] = df['text_corrected'].apply(lambda x : remove_punctuation(x, regular_punct)).apply(lambda x : \" \".join(x.split()))\n","\n","\n","\n","from nltk.stem.porter import PorterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","\n","def stem_sentences(sentence):\n","    tokens = sentence.split()\n","    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n","    return ' '.join(stemmed_tokens)\n","\n","df['porter_original'] = df['text_corrected'].apply(stem_sentences)\n","df['porter_punct'] = df['punct'].apply(stem_sentences)\n","\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","#example text text = 'What can I say about this place. The staff of these restaurants is nice and the eggplant is not bad'\n","\n","class Splitter(object):\n","    \"\"\"\n","    split the document into sentences and tokenize each sentence\n","    \"\"\"\n","    def __init__(self):\n","        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n","        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n","\n","    def split(self,text):\n","        \"\"\"\n","        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n","        \"\"\"\n","        # split into single sentence\n","        sentences = self.splitter.tokenize(text)\n","        # tokenization in each sentences\n","        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n","        return tokens\n","\n","\n","class LemmatizationWithPOSTagger(object):\n","    def __init__(self):\n","        pass\n","    def get_wordnet_pos(self,treebank_tag):\n","        \"\"\"\n","        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n","        \"\"\"\n","        if treebank_tag.startswith('J'):\n","            return wordnet.ADJ\n","        elif treebank_tag.startswith('V'):\n","            return wordnet.VERB\n","        elif treebank_tag.startswith('N'):\n","            return wordnet.NOUN\n","        elif treebank_tag.startswith('R'):\n","            return wordnet.ADV\n","        else:\n","            # As default pos in lemmatization is Noun\n","            return wordnet.NOUN\n","\n","    def pos_tag(self,tokens):\n","        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n","        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n","\n","        # lemmatization using pos tagg   \n","        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n","        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n","        return pos_tokens\n","\n","lemmatizer = WordNetLemmatizer()\n","splitter = Splitter()\n","lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n","\n","#step 1 split document into sentence followed by tokenization\n","\n","def lemmatise(text):\n","  tokens = splitter.split(text)\n","  #step 2 lemmatization using pos tagger \n","  lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n","  ans = \"\"\n","  for sentence in lemma_pos_token:\n","    for word in sentence:\n","      ans += word[1] + \" \"\n","\n","  return ans[:-1]\n","\n","\n","df['lemma_original'] = df['text_corrected'].apply(lemmatise)\n","df['lemma_punct'] = df['punct'].apply(lemmatise)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7921,"status":"ok","timestamp":1655712256117,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"469XHQBQFO1B","outputId":"7e6f3bbb-40ae-4f92-ba95-9bb60b6aa07b"},"outputs":[],"source":["! pip install scikit-multilearn\n","! pip install iterative-stratification"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3252,"status":"ok","timestamp":1655712259361,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"3nDvxhDSFznq"},"outputs":[],"source":["from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, BatchNormalization, Input\n","import tensorflow as tf\n","\n","\n","def create_model(input_dim, output_dim, dropout, hidden_size):\n","\n","    inp = Input(input_dim)\n","    x = Dense(hidden_size, activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros')(inp)\n","    x = Dropout(dropout)(x, training=True)\n","    out = Dense(output_dim, activation='sigmoid',kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros')(x)\n","    model = Model(inputs = inp, outputs = out)\n","\n","    #optimizer\n","    #model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=['categorical_accuracy'])\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1655712259361,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"Va4enFqlWrSd"},"outputs":[],"source":["from ast import literal_eval\n","\n","df['labels'] = df['labels'].apply(literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1655712259362,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"4hnkL-ZBDTYp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655712259362,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"aNC8e0OLIa04"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","#### one hote enconding on the labels ########\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(df['labels'])\n","\n","#### get the text column\n","\n","X = df['text_processed']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655712259363,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"4R5xqLYTIczb","outputId":"929ea56d-df90-4012-cf0d-5dea469a315f"},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655712259363,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"LuyT0rh_AdzK"},"outputs":[],"source":["import os\n","import random\n","def tf_seed(seed=0):\n","\tos.environ['PYTHONHASHSEED'] = str(seed)\n","\t# For working on GPUs from \"TensorFlow Determinism\"\n","\tos.environ[\"TF_DETERMINISTIC_OPS\"] = str(seed)\n","\tnp.random.seed(seed)\n","\trandom.seed(seed)\n","\ttf.random.set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655712259364,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"fpRdT0uQBg6g"},"outputs":[],"source":["tf_seed()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655712259364,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"wdW8dBiwCBIZ"},"outputs":[],"source":["from tensorflow.python.ops.script_ops import numpy_function\n","from keras import backend as K\n","\n","def calculating_class_weights(y_true):\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    print(number_dim)\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        print(compute_class_weight(class_weight='balanced', classes= [0,1], y= y_true[:, i]))\n","        weights[i] = compute_class_weight(class_weight='balanced', classes= [0.,1.], y= y_true[:, i])\n","\n","    print(weights)\n","    return weights\n","\n","\n","def get_weighted_loss(weights):\n","    def weighted_loss(y_true, y_pred):\n","        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n","    return weighted_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1327,"status":"ok","timestamp":1655712260681,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"1qTeUsWsc_KE","outputId":"d76924e6-7cf8-4fd9-b94a-c1c78e23b5d3"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","import numpy as np\n","\n","def average_sentences(X):\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      current_embedding = embed(sentences).numpy()\n","      '''\n","      diff = number -len(current_embedding)\n","      if diff > 0:\n","          padding = np.zeros((diff, 512))\n","          current_embedding = np.concatenate((padding, current_embedding), axis=0)\n","      '''\n","      new_X.append(np.mean(current_embedding, axis=0))\n","      \n","    \n","    return np.array(new_X)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3206,"status":"ok","timestamp":1655712263880,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"2v-NC29WFZXM","outputId":"ea134530-5932-40ae-8399-f69f211bfc0c"},"outputs":[],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178223,"status":"ok","timestamp":1655712442099,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"FFzikVatFRiF","outputId":"f4b7c03f-2b2e-41b4-9b20-a3419ac62af8"},"outputs":[],"source":["import gensim\n","import gensim.downloader\n","embedding = gensim.downloader.load('glove-twitter-50')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"elapsed":72798,"status":"error","timestamp":1655712514890,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"_72egRHko-zR","outputId":"dd08ae61-049a-40f4-a4ca-4183afc073e5"},"outputs":[],"source":["embedding_twitter200 = gensim.downloader.load('glove-twitter-200')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1655712514404,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"J6XCjQe_abCS"},"outputs":[],"source":["embedding_word2vec = gensim.downloader.load('word2vec-google-news-300')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1655712514405,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"rfUnTnuDvOdS"},"outputs":[],"source":["embedding_fast = gensim.downloader.load('fasttext-wiki-news-subwords-300')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47691,"status":"ok","timestamp":1655712564430,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"nxenNfBl5Vaf"},"outputs":[],"source":["import tensorflow_hub as hub\n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655712564431,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"JrzZ8qyB5XeG"},"outputs":[],"source":["def vectorize_sentences(X):\n","    embeded_tweets = embed(X).numpy()\n","    return embeded_tweets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655712564431,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"GWGwKOppJWz2"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","\n","def get_embedding_result(word, embedding):\n","  return embedding[word] if word in embedding else 1\n","\n","\n","def get_gloVe(X_train, X_test, embeddings_index):\n","  X_train_vector = []\n","  for text in X_train:\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_train_vector.append(np.average(words, axis = 0))\n","\n","  X_test_vector = []\n","  for text in X_test:\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_test_vector.append(np.average(words, axis = 0))\n","\n","    \n","  return X_train_vector, X_test_vector\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def get_gloVe_tf_idf(X_train, X_test, embeddings_index):\n","  tfidf = TfidfVectorizer()\n","  tfidf.fit(X_train)\n","  idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n","\n","  train_vector = []\n","  for text in X_train:\n","    weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    train_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  test_vector = []\n","  for text in X_test:\n","    weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    test_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  return train_vector, test_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":134198,"status":"error","timestamp":1655712698617,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"KTglzIoRIMOf","outputId":"c2b4de27-83d1-49e6-ff56-bff73b982be2"},"outputs":[],"source":["from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, hamming_loss, jaccard_score, precision_score, recall_score\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from collections import defaultdict\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","counter = 1\n","store = defaultdict(float)\n","\n","for train_index, test_index in msss.split(np.array(X), np.array(y)):\n","  import warnings\n","  warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","\n","  for dropout in [0., 0.1, 0.3, 0.5]:\n","    for hidden_layer in [16, 32, 64, 128, 256]:\n","      #for k_values in [200, 300, 500, 1000]:\n","\n","\n","   ####### performing the different splits here #########\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","\n","        text_clf = Pipeline([\n","              ('vect', CountVectorizer()),\n","              #('tfidf', TfidfTransformer()),\n","              #('smote', MLSmote()),\n","              #('fs', SelectKBest(chi2, k= k_values)),\n","              #('debug', Debug())\n","        ])\n","\n","        #X_train = text_clf.fit_transform(X_train, y_train).toarray()\n","        #X_test = text_clf.transform(X_test).toarray()\n","        # boy2\n","        X_train, X_test = get_gloVe_tf_idf(X_train, X_test, embedding)\n","\n","        #X_train = vectorize_sentences(X_train)\n","        #X_test = vectorize_sentences(X_test)\n","        X_train = np.array(X_train)\n","        X_test = np.array(X_test)\n","\n","        print(X_train.shape)\n","\n","        model = create_model(X_train.shape[1], y.shape[1], dropout=dropout, hidden_size=hidden_layer)\n","        #print(model.summary())\n","        class_weights = calculating_class_weights(y_train)\n","        print(class_weights)\n","        #print(class_weights)\n","        opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","\n","        model.compile(optimizer=opt, loss=get_weighted_loss(class_weights))\n","        print(y_train.shape, y_test.shape)\n","\n","        model.fit(X_train, tf.cast(y_train, tf.float32), epochs=20, validation_data=(X_test, tf.cast(y_test, tf.float32)))\n","\n","        y_predict = model.predict(X_test)\n","        thresholds = np.arange(0, 1, 0.005)\n","        from sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score, accuracy_score # we can use gmean\n","\n","        def to_labels(pos_probs, threshold):\n","          return (pos_probs >= threshold).astype('int')\n","      \n","\n","        mc_predictions = []\n","\n","        RANGE_VALUES = 100\n","        for i in range(RANGE_VALUES):\n","          y_p = model.predict(X_test)\n","          mc_predictions.append(y_p)\n","        \n","        accs = []\n","        f1_scores = []\n","        recall_scores = []\n","        precision_scores = []\n","        matrix_sum = np.zeros((y.shape[1],2,2))\n","\n","        for y_p in mc_predictions:\n","          #f1_score(y_test, to_labels(y_p, t), average=\"weighted\") * 0.5 + \n","          scores = [accuracy_score(y_test, to_labels(y_p, t)) for t in thresholds]\n","          ix = np.argmax(scores)\n","          y_predict = y_p > thresholds[ix]\n","          accs.append(accuracy_score(y_test, y_predict))\n","          f1_scores.append(f1_score(y_test, y_predict, average=\"weighted\"))\n","          recall_scores.append(recall_score(y_test, y_predict, average=\"weighted\"))\n","          precision_scores.append(precision_score(y_test, y_predict, average=\"weighted\"))\n","          multi_matrix = multilabel_confusion_matrix(y_test, y_predict)\n","          matrix_sum = np.sum([matrix_sum, multi_matrix], axis=0)\n","          #print(multi_matrix)\n","          #print(matrix_sum)\n","\n","        #print(\"Matrix confusion\\n\", matrix_sum/ (RANGE_VALUES))\n","\n","        #\n","        #print(\"MC accuracy: {:.1%}\".format(sum(accs)/len(accs)))\n","        #print(\"F1 weighted accuracy: {:.1%}\".format(sum(f1_scores)/len(f1_scores)))\n","        #print(\"*\" * 10)\n","        print(counter, \"done\", sum(f1_scores)/len(f1_scores))\n","        counter += 1\n","        #print(\"Precision weighted accuracy: {:.1%}\".format(sum(precision_scores)/len(precision_scores)))\n","        #print(\"Recall weighted accuracy: {:.1%}\".format(sum(recall_scores)/len(recall_scores)))\n","\n","        store[sum(f1_scores)/len(f1_scores)] = [dropout, hidden_layer]\n","        labels = mlb.classes_\n","\n","        conf_mat_dict={}\n","        from sklearn.metrics import confusion_matrix\n","\n","\n","        #print(\"accuracy: \", accuracy_score(y_test, y_predict))\n","        #print(\"hamming loss: \", hamming_loss(y_test, y_predict))\n","        #print(\"jaccard score: \", jaccard_score(y_test, y_predict, average='weighted'))\n","        #print(multilabel_confusion_matrix(y_test, y_predict))\n","        #print(classification_report(y_test, y_predict))\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1655712698613,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"51D9RJl8IpD6"},"outputs":[],"source":["store\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1655712698614,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"f5sFpISgli5t"},"outputs":[],"source":["store"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48005,"status":"ok","timestamp":1655713485242,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"NR4vB_dxfC_l","outputId":"41e94992-410d-464b-c19d-d8c1b136ef7a"},"outputs":[],"source":["from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.multiclass import OneVsRestClassifier ### does not take into account label correlations\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, hamming_loss, jaccard_score\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","#from sklearn.multioutput import ClassifierChain\n","\n","from sklearn.utils import compute_class_weight\n","from skmultilearn.problem_transform import BinaryRelevance\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","from skmultilearn.problem_transform import ClassifierChain, LabelPowerset\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from skmultilearn.cluster import NetworkXLabelGraphClusterer\n","from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n","from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","\n","# linear is nice 0.47\n","# poly 0.37\n","# rbf 0.34\n","# sigmoid 0.43\n","\n","#from skmultilearn.ext import Keras\n","\n","from skmultilearn.adapt import MLkNN\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","################ run the pipeline #####################\n","\n","#### RF E LR\n","for train_index, test_index in msss.split(np.array(X), np.array(y)):\n","  #index = - 1\n","  #for clf in [BinaryRelevance(), ClassifierChain(), LabelPowerset()]:\n","   #index = index + 1\n","   #for tfidf in [True, False]:\n","    #for embedding_str in  [\\\n","    #   'word2vec-google-news-300'    ,\n","    #  'fasttext-wiki-news-subwords-300', \n","   #   'glove-twitter-50', #done\n","    #  'glove-twitter-200' # done\n","    #  ]:\n","\n","      #embedding = gensim.downloader.load(embedding_str)\n","    #for other_thing in [BinaryRelevance(), ClassifierChain(), LabelPowerset()]:\n","     #for tf_idf in [True, False]:\n","\n","      import warnings\n","      warnings.simplefilter(action='ignore', category=FutureWarning)\n","      ####### performing the different splits here #########\n","      X_train, X_test = X[train_index], X[test_index]\n","      y_train, y_test = y[train_index], y[test_index]\n","\n","\n","      #if tfidf:\n","      #else:\n","      #  X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","\n","\n","      from skmultilearn.problem_transform import BinaryRelevance\n","\n","\n","\n","      ### model xgboost ###~\n","      from xgboost import XGBClassifier\n","      from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","      \n","      KERAS_PARAMS = dict(epochs=15, batch_size=2, verbose=1)\n","        \n","\n","\n","      #### model b)\n","      from skmultilearn.adapt import BRkNNaClassifier # 1 parameter estimation\n","\n","      ### model c)\n","     \n","      #clf = GridSearchCV(MLkNN(), parameters, scoring=score)\n","\n","      ### model d) # alternative to for example classifier chain\n","      from sklearn.naive_bayes import GaussianNB\n","     \n","\n","      #classifier = RakelD(\n","      #    base_classifier= SVC(class_weight='balanced', random_state=42, kernel=kernels[0], C=2),\n","      #    base_classifier_require_dense=[True, True],\n","      #    labelset_size=4\n","      #)\n","\n","      ### model e) # another alternative\n","      from skmultilearn.ensemble import MajorityVotingClassifier\n","      from skmultilearn.cluster import FixedLabelSpaceClusterer\n","      from skmultilearn.problem_transform import ClassifierChain\n","      from sklearn.naive_bayes import GaussianNB\n","\n","\n","\n","      ### labels embeddings #### => nao tou a conseguir por a dar\n","      import joblib\n","      import sys\n","      sys.modules['sklearn.externals.joblib'] = joblib\n","      #from skmultilearn.embedding import CLEMS, EmbeddingClassifier\n","\n","\n","\n","      # Compile model\n","      ###### vectorize the text for training and test seperately #######\n","      from sklearn.preprocessing import FunctionTransformer\n","\n","\n","      ### perform data augmentation of the training data prior to the preprocessing phase ###\n","      #from textattack.augmentation import CheckListAugmenter\n","      #from textattack.augmentation import EasyDataAugmenter, EmbeddingAugmenter\n","\n","\n","      #X_min, y_min = get_minority_samples(X_train, pd.DataFrame(y_train, columns=mlb.classes_))\n","      #print(\"Size of the minority samples\", X_min.shape[0])\n","      number=1\n","      #checklist_augmenter = EmbeddingAugmenter(pct_words_to_swap=0.1, transformations_per_example=number)\n","\n","      #X_train, y_train = paraphrase_augmentation(X_train, y_train)\n","      \n","      #X_train, y_train = textattack_data_augment(X_train, y_train, checklist_augmenter, number)\n","\n","      #X_train, y_train = augment_data(X_train, y_train)\n","\n","      X_train = np.array(X_train)\n","      y_train = np.array(y_train)\n","\n","\n","\n","      from sklearn.neighbors import KNeighborsClassifier\n","\n","      #print(\"New dataset \", X_train.shape, \"and \", y_train.shape)\n","      from sklearn.tree import DecisionTreeClassifier\n","      #classifier = DecisionTreeClassifier()\n","      from sklearn.naive_bayes import GaussianNB\n","\n","      from sklearn.neighbors import KNeighborsClassifier\n","      from sklearn.linear_model import LogisticRegression\n","\n","      \n","      print(text_clf.get_params().keys())\n","      from sklearn.naive_bayes import MultinomialNB\n","      '''\n","      '''\n","      parameters2 = {\n","        \"vect__analyzer\": ['char_wb', 'word'],\n","        \"vect__ngram_range\": [ (1,2), (1,1), (1,3)],\n","        'tfidf__use_idf': [ True, False],\n","        'tfidf__norm': [ 'l2', 'l1'],\n","        \"criterion\": ['gini', 'entropy'],\n","        #\"class_weight\": [\"balanced\"],\n","        #\"ccp_alpha\": [0, 0.005, 0.01, 0.02]\n","\n","        #\"classifier\":[LogisticRegression(class_weight=\"balanced\")],\n","        #\"classifier__penalty\":['l1', 'l2'],\n","        #\"classifier__C\": [0.1, 1, 10, 100],\n","        #\"classifier__solver\": ['liblinear']\n","        #\"clf__classifier__loss\": [\"hinge\", \"squared_hinge\"],\n","        #\"clf__classifier__penalty\": ['l1', 'l2']\n","        #\"clf__classifier__var_smoothing\": [1e-8, 1e-9, 1e-10]\n","        #\"clf__n_neighbors\":[3, 5, 10],\n","        #\"clf__weights\": ['uniform', 'distance'],\n","        #\"clf__p\": [1,2 ]\n","      }\n","\n","\n","      #X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","\n","      \n","      parameters3= {\n","        # DT\n","        #\"criterion\": ['gini', 'entropy'],\n","        #\"class_weight\": [\"balanced\"],\n","        #\"ccp_alpha\": [0, 0.005, 0.01, 0.02]\n","\n","        # \n","\n","        #\"classifier\": [GaussianNB()],\n","        #\"classifier__var_smoothing\": [1e-2, 1e-3],\n","\n","        #\"n_neighbors\":[3, 5, 10],\n","        #\"weights\": ['uniform', 'distance'],\n","        #\"p\": [1,2] ,\n","\n","\n","        #\"classifier\": [SVC()],\n","        #\"classifier__kernel\": ['linear'],\n","        #\"classifier__C\": [20],\n","        #\"classifier__class_weight\": [\"balanced\"]\n","\n","        #\"classifier\":[LogisticRegression(class_weight=\"balanced\")],\n","        #\"classifier__penalty\":['l1', 'l2'],\n","        #\"classifier__C\": [0.1, 1, 10, 100],\n","        #\"classifier__solver\": ['liblinear']\n","\n","\n","        #\"bootstrap\": [True, False],\n","        #\"max_depth\": [10, 20],\n","        #\"n_estimators\": [400, 600],\n","        #\"class_weight\":['balanced']\n","\n","\n","      \n","        #\"classifier__alpha\": [1e-2, 1e-3]\n","        #\"vect__ngram_range\": ((1, 1), (1, 2)),\n","\n","\n","        #'tfidf__use_idf': (True, False),\n","        #'tfidf__norm': ['l2', 'l1'],\n","        #\"clf__n_neighbors\":[3, 5, 10],\n","        #\"clf__weights\": ['uniform', 'distance'],\n","        #\"clf__p\": [1,2 ]\n","        #'clf__bootstrap': [True, False],\n","        #'clf__max_depth': [10, 30], #40, 50, 60, 100, None\n","        #'clf__n_estimators': [ 200, 300] # , 400, 600, 800, 1000\n","        }\n","\n","      from sklearn.metrics import make_scorer\n","\n","      #if tf_idf:\n","      #else:\n","      #  X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","      #X_train = vectorize_sentences(X_train)\n","      #X_test = vectorize_sentences(X_test)\n","      #X_train = np.array(X_train)\n","      #X_test = np.array(X_test)\n","\n","\n","            ###### perform feature engineering and append those features #####\n","      from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler \n","      '''\n","      X_train_length = get_length_features(X_train)\n","      X_test_length = get_length_features(X_test)\n","\n","      X_train_sentiment = calc_sentiment_scores(X_train)\n","      X_test_sentiment = calc_sentiment_scores(X_test)\n","\n","      X_train_read = scores[train_index]\n","      X_test_read = scores[test_index]\n","\n","      ##### topic values ########\n","\n","      X_train_topic = topic_values[train_index]\n","      X_test_topic = topic_values[test_index]\n","\n","      ######## ner part if we want #####\n","\n","      X_train_ner = append_ner_features(df2.iloc[train_index], 'tags', tags_set)[tags_set]\n","      X_test_ner = append_ner_features(df2.iloc[test_index], 'tags', tags_set)[tags_set]\n","\n","      ########### pos ##############\n","      X_train_pos = PoS_counter(X_train)\n","      X_test_pos = PoS_counter(X_test)\n","\n","\n","      feature_train = np.hstack(( \n","          \n","          X_train_read,\n","     \n","                      X_train_topic,\n","                   \n","                     ))\n","      \n","      normalizer = MinMaxScaler()\n","      feature_train = normalizer.fit_transform(feature_train)\n","      feature_train = sparse.csr_matrix(feature_train)\n","\n","\n","      feature_test = np.hstack(( \n","          X_test_read,\n","        \n","                      X_test_topic ,\n","                   \n","                   ))\n","      \n","      normalizer = MinMaxScaler()\n","      feature_test = normalizer.fit_transform(feature_test)\n","      feature_test = sparse.csr_matrix(feature_test)\n","\n","      '''\n","   \n","\n","\n","      from sklearn.feature_extraction.text import TfidfVectorizer\n","      \n","      #vectorizer = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(1,2), use_idf=True, norm='l2')\n","      #X_train = vectorizer.fit_transform(X_train)\n","      #X_test = vectorizer.transform(X_test)\n","\n","      X_train  = average_sentences(X_train)\n","      X_test = average_sentences(X_test)\n","      #X_train, X_test = get_gloVe(X_train, X_test, embedding_fast)\n","      #X_train  = vectorize_sentences(X_train)\n","      #X_test = vectorize_sentences(X_test)\n","      # hereboy\n","\n","      parameters = {\n","        \"classifier\":[LogisticRegression(class_weight=\"balanced\")],\n","        \"classifier__penalty\":['l1', 'l2'],\n","        \"classifier__C\": [0.1, 1, 10, 100],\n","        \"classifier__solver\": ['liblinear']\n","      }\n","\n","      \n","      '''\n","      X_train = sparse.hstack((\n","          sparse.csr_matrix(X_train),\n","          feature_train\n","      ))\n","      X_test = sparse.hstack((\n","          sparse.csr_matrix(X_test),\n","          feature_test\n","      ))\n","\n","      print(X_train.shape)\n","      \n","      '''\n","\n","      \n","    \n","      #clf = ClassifierChain(SVC(grid_search, parameters, class_weight='balanced', kernel='linear'))\n","\n","    \n","\n","\n","      for al in [BinaryRelevance(), ClassifierChain(), LabelPowerset()]:\n","\n","        grid_search = GridSearchCV(al, parameters, verbose=1, cv=5)\n","        grid_search.fit(X_train, y_train)\n","\n","        #print(\"val\", index)\n","        print(\"Best score: %0.3f\" % grid_search.best_score_)\n","        print(\"Best parameters set:\")\n","        best_parameters = grid_search.best_estimator_.get_params()\n","        print(parameters.keys())\n","        for param_name in sorted(parameters.keys()):\n","          print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","\n","\n","        y_predict = grid_search.predict(X_test)\n","        # Find the best parameters for both the feature extraction and the\n","        # classifier\n","\n","        ######## APPLY SMOTED IF WANTED  ###################\n","\n","        #vectorizered_text = text_clf.fit_transform(X_train, y_train)\n","\n","        ['fasttext-wiki-news-subwords-300',\n","        'word2vec-google-news-300', #done\n","        'glove-twitter-50', #done\n","        'glove-twitter-200' # done\n","        ]\n","\n","\n","\n","        \n","    \n","        '''\n","        ### readability scores ###\n","\n","        X_train_read = scores[X_train.index]\n","        X_test_read = scores[X_test.index]\n","\n","        ##### topic values ########\n","\n","        X_train_topic = topic_values[X_train.index]\n","        X_test_topic = topic_values[X_test.index]\n","\n","        ######## ner part if we want #####\n","\n","        X_train_ner = append_ner_features(df2.iloc[X_train.index], 'tags', tags_set)[tags_set]\n","        X_test_ner = append_ner_features(df2.iloc[X_test.index], 'tags', tags_set)[tags_set]\n","\n","        print(\"ner\", X_train_ner.shape, X_test_ner.shape)\n","\n","        X_train_pos = PoS_counter(X_train)\n","        X_test_pos = PoS_counter(X_test)\n","        '''\n","        #embedding = gensim.downloader.load('glove-twitter-50')\n","\n","        #vectorizered_text, test_text = get_gloVe_tf_idf(X_train, X_test, embedding)\n","\n","        #### here for the others\n","        '''\n","        X_train = text_clf.fit_transform(X_train)\n","        test_text = text_clf.transform(X_test)\n","\n","        print(np.array(X_train).shape, np.array(test_text).shape)\n","        X_train = pd.DataFrame(data = X_train)\n","        y_train =  pd.DataFrame(y_train, columns=mlb.classes_)\n","        '''\n","\n","\n","        \n","\n","        #normalizer = MaxAbsScaler ()\n","        #X_train = normalizer.fit_transform(X_train)\n","        #X_test = normalizer.fit(X_test)\n","\n","        #features = sparse.csr_matrix(features)\n","\n","\n","        '''\n","        PERCENTAGE = 0 # 0 means that is not applied\n","        NR_NEIGHBORS = 5\n","\n","        vectorizered_text = text_clf.fit_transform(X_train, y_train)\n","        \n","        df_X = pd.DataFrame(data = vectorizered_text.toarray())\n","        df_y =  pd.DataFrame(y_train, columns=mlb.classes_)\n","\n","        X_sub, y_sub = get_minority_samples(df_X, df_y)\n","\n","        #print(\"Size of the minority samples\", X_sub.shape[0])\n","\n","        X_res, y_res = MLSMOTE(X_sub, y_sub, int(df_X.shape[0] * PERCENTAGE), NR_NEIGHBORS)\n","        '''\n","        ############## train the classifier\n","\n","        #X_train_augmented = pd.concat([df_X, X_res])\n","        #classifier.fit(pd.concat([df_X, X_res]), pd.concat([df_y, y_res]))\n","        #classifier.fit(X_train, y_train)\n","        ###### perform model validation #####################\n","\n","\n","        #y_predict = classifier.predict(X_test)\n","\n","        ### trying monte carlo ####\n","        '''\n","        test_text = text_clf.transform(X_test)\n","        mc_predictions = []\n","\n","        for i in range(20):\n","          y_p = classifier.predict(test_text)\n","          mc_predictions.append(y_p)\n","        \n","        accs = []\n","        for y_p in mc_predictions:\n","          acc = accuracy_score(y_pred=y_p, y_true=y_test)\n","          accs.append(acc)\n","\n","        print(\"MC accuracy: {:.1%}\".format(sum(accs)/len(accs)))\n","        '''\n","        #print(\"embedding\", embedding_str)\n","        #print(\"tfidf\", tfidf)\n","        from sklearn.metrics import f1_score\n","        \n","        print(\"accuracy: \", accuracy_score(y_test, y_predict))\n","        print(\"hamming loss: \", hamming_loss(y_test, y_predict))\n","        print( round(hamming_loss(y_test, y_predict),3), round(jaccard_score(y_test, y_predict, average='weighted'),3))\n","        print( \" + \".join(   [str(round(x,2)) for x in f1_score(y_test, y_predict, average=None) ]       ), \"(\", round(f1_score(y_test, y_predict, average=\"weighted\"),2), \")\")\n","        print(\"jaccard score: \", jaccard_score(y_test, y_predict, average='weighted'))\n","        print(multilabel_confusion_matrix(y_test, y_predict))\n","        print(classification_report(y_test, y_predict))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1655712698616,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"MD3pXNDWbK0D"},"outputs":[],"source":["X_train"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNgPfyL/Q0fZA+fc1Qx+osi","name":"NN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
