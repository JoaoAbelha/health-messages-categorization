{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24558,"status":"ok","timestamp":1654894840047,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"DczsBkqK3w8h","outputId":"78cfa2d7-ea29-41fa-b074-4e4b6ed350fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":993,"status":"ok","timestamp":1654894841034,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"-uK6M5LW37ru"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('/content/gdrive/My Drive/Mestrado/first_level_dataset.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2679,"status":"ok","timestamp":1654894843708,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"JjQnTfbt39YF","outputId":"7298bbc1-fec3-4167-8a01-fc9edee7e9af"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["import string\n","regular_punct = list(string.punctuation)\n","def remove_punctuation(text,punct_list):\n","    for punc in punct_list:\n","        if punc in text:\n","            text = text.replace(punc, ' ' )\n","    return text.strip()\n","\n","#df['punct'] = df['to_save'].apply(lambda x : remove_punctuation(x, regular_punct)).apply(lambda x : \" \".join(x.split()))\n","\n","\n","\n","from nltk.stem.porter import PorterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","\n","def stem_sentences(sentence):\n","    tokens = sentence.split()\n","    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n","    return ' '.join(stemmed_tokens)\n","\n","#df['porter_original'] = df['to_save'].apply(stem_sentences)\n","#df['porter_punct'] = df['punct'].apply(stem_sentences)\n","\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","#example text text = 'What can I say about this place. The staff of these restaurants is nice and the eggplant is not bad'\n","\n","class Splitter(object):\n","    \"\"\"\n","    split the document into sentences and tokenize each sentence\n","    \"\"\"\n","    def __init__(self):\n","        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n","        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n","\n","    def split(self,text):\n","        \"\"\"\n","        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n","        \"\"\"\n","        # split into single sentence\n","        sentences = self.splitter.tokenize(text)\n","        # tokenization in each sentences\n","        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n","        return tokens\n","\n","\n","class LemmatizationWithPOSTagger(object):\n","    def __init__(self):\n","        pass\n","    def get_wordnet_pos(self,treebank_tag):\n","        \"\"\"\n","        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n","        \"\"\"\n","        if treebank_tag.startswith('J'):\n","            return wordnet.ADJ\n","        elif treebank_tag.startswith('V'):\n","            return wordnet.VERB\n","        elif treebank_tag.startswith('N'):\n","            return wordnet.NOUN\n","        elif treebank_tag.startswith('R'):\n","            return wordnet.ADV\n","        else:\n","            # As default pos in lemmatization is Noun\n","            return wordnet.NOUN\n","\n","    def pos_tag(self,tokens):\n","        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n","        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n","\n","        # lemmatization using pos tagg   \n","        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n","        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n","        return pos_tokens\n","\n","lemmatizer = WordNetLemmatizer()\n","splitter = Splitter()\n","lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n","\n","#step 1 split document into sentence followed by tokenization\n","\n","def lemmatise(text):\n","  tokens = splitter.split(text)\n","  #step 2 lemmatization using pos tagger \n","  lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n","  ans = \"\"\n","  for sentence in lemma_pos_token:\n","    for word in sentence:\n","      ans += word[1] + \" \"\n","\n","  return ans[:-1]\n","\n","\n","#df['lemma_original'] = df['to_save'].apply(lemmatise)\n","#df['lemma_punct'] = df['punct'].apply(lemmatise)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8637,"status":"ok","timestamp":1654894852335,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"xEkT4kF83_F5","outputId":"ee811975-2fdb-41e8-b6a0-f49380f67488"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-multilearn\n","  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n","\u001b[K     |████████████████████████████████| 89 kB 3.3 MB/s \n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting iterative-stratification\n","  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003eiterative-stratification) (1.1.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-\u003eiterative-stratification) (3.1.0)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.7\n"]}],"source":["! pip install scikit-multilearn\n","! pip install iterative-stratification"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1654894852336,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"dnl9nI6O4Bu-"},"outputs":[],"source":["from ast import literal_eval\n","\n","df['labels'] = df['labels'].apply(literal_eval)"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1654900604148,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"JxGPj0n04pTX"},"outputs":[],"source":["from tensorflow.keras import layers, regularizers\n","from tensorflow.keras import Sequential\n","\n","def create_model(input_size, output):\n","  model3 = Sequential()\n","  model3.add(layers.Conv1D(8, 5, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),\\\n","                           bias_regularizer=regularizers.l2(2e-3), input_shape=input_size) )\n","  model3.add(layers.MaxPooling1D(5))\n","  model3.add(layers.Conv1D(8, 5, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n","  model3.add(layers.GlobalMaxPooling1D())\n","  model3.add(layers.Dense(output,activation='sigmoid'))\n","  return model3\n"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1654900613690,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"Dqw0JhakD6YZ"},"outputs":[],"source":["# univariate lstm example\n","from numpy import array\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU, Input,  Bidirectional\n","from keras.layers import Dense, Dropout, LeakyReLU, Conv1D, MaxPool1D, GlobalMaxPool1D\n","from keras.regularizers import l1,l2\n","\n","def lstm(input_shape, output_shape):\n","  # define model\n","  model = Sequential()\n","  #model.add(Conv1D(filters=8, kernel_size=3,strides=1, padding=\"causal\", activation=\"relu\", input_shape=(7, 128), activity_regularizer=l1(0.0001) ) )\n","  #model.add(Conv1D(filters=4, kernel_size=3,strides=1, padding=\"causal\", activation=\"relu\", activity_regularizer=l1(0.0001)) )\n","  #model.add(Conv1D(filters=8, kernel_size=3,strides=1, padding=\"causal\", activation=\"relu\") )\n","  #model.add(MaxPool1D())\n","  model.add(Input(shape=input_shape))\n","  #model.add((LSTM(3, activation=LeakyReLU(alpha=0.1), return_sequences=True, activity_regularizer=l2(0.0001))))\n","  model.add((LSTM(3, activation=LeakyReLU(alpha=0.1), return_sequences=False, activity_regularizer=l2(0.0001))))\n","\n","  model.add(Dense(48))\n","  model.add(Dropout(0.3))\n","  model.add(Dense(output_shape, activation='sigmoid'))\n","  return model\n","  #model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1654894856547,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"lErNmr9q41em"},"outputs":[],"source":["from tensorflow.python.ops.script_ops import numpy_function\n","from keras import backend as K\n","\n","def calculating_class_weights(y_true):\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    print(number_dim)\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        print(compute_class_weight(class_weight='balanced', classes= [0,1], y= y_true[:, i]))\n","        weights[i] = compute_class_weight(class_weight='balanced', classes= [0.,1.], y= y_true[:, i])\n","\n","    print(weights)\n","    return weights\n","\n","\n","def get_weighted_loss(weights):\n","    def weighted_loss(y_true, y_pred):\n","        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n","    return weighted_loss"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1414,"status":"ok","timestamp":1654894857953,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"RvXJWJJw455G","outputId":"720fe77c-0026-4900-f374-0b907a65d904"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: scipy\u003e=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: numpy\u003e=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: smart-open\u003e=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n","Requirement already satisfied: six\u003e=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"]}],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":503279,"status":"ok","timestamp":1654898232996,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"iQFmmrh34872","outputId":"e6ad5b72-4566-49b2-ba76-111a014aebd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 758.5/758.5MB downloaded\n"]}],"source":["import gensim\n","import gensim.downloader\n","embedding = gensim.downloader.load('glove-twitter-200')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1654895120301,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"l92RLUnR4-4e"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","\n","def get_embedding_result(word, embedding):\n","  return embedding[word] if word in embedding else embedding[\"\u003cUNK\u003e\"]\n","\n","\n","def get_gloVe(text_train, embeddings_index, sequence_size):\n","  X_train_vector = []\n","\n","  for word in word_tokenize(text_train):\n","    if len(X_train_vector) \u003e= sequence_size:\n","      break\n","    if word in embeddings_index:\n","        X_train_vector.append(embedding[word])\n","\n","  zeros= []\n","  for i in range(len(X_train_vector), sequence_size):\n","    zeros.append(np.zeros(X_train_vector[0].shape))\n","    #X_train_vector.append(np.average(words, axis = 0))\n","  X_train_vector = zeros + X_train_vector\n","\n","  return X_train_vector\n","\n","\n","def get_embeddings(X_train, X_test,  embeddings_index, sequence_size):\n","\n","  train_result = []\n","  for text in X_train:\n","    train_result.append(get_gloVe(text, embeddings_index, sequence_size))\n","  \n","  test_result = []\n","  for text in X_test:\n","    test_result.append(get_gloVe(text, embeddings_index, sequence_size))\n","\n","  return train_result, test_result"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1654895120301,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"RipfOkX34EgV"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","#### one hote enconding on the labels ########\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(df['labels'])\n","\n","#### get the text column\n","\n","X = df['text']"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1654895120302,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"fQCHUS8OGJ-K"},"outputs":[],"source":["sizes = df['text'].apply(len)\n","sequence_size_first = int(np.array(sizes).mean())\n","sequence_size_second = int(np.median(sizes))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1654895120302,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"zCOsBEgQ4ApX","outputId":"d29a5db6-2c27-4166-9c50-d2f201140b3c"},"outputs":[{"data":{"text/plain":["581"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["sequence_size_first"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1654895120302,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"yU0a7AeL8d6X","outputId":"3fe37ece-35ee-4d18-dd7f-c1ef8b17a626"},"outputs":[{"data":{"text/plain":["((453,), (453, 3))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":39112,"status":"ok","timestamp":1654900120135,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"Ht4s11vUuVAp"},"outputs":[],"source":["import tensorflow_hub as hub\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","\n","def vectorize_sentences(X, number):\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      current_embedding = embed(sentences[:number]).numpy()\n","      \n","      diff = number -len(current_embedding)\n","      if diff \u003e 0:\n","          padding = np.zeros((diff, 512))\n","          current_embedding = np.concatenate((padding, current_embedding), axis=0)\n","      new_X.append(current_embedding)\n","    \n","    return np.array(new_X)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1654900583530,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"PZNzjP8Bu6-B"},"outputs":[],"source":["sentence_length = int(np.mean(df['text'].apply(lambda x : len(sent_tokenize(x)))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Wsebm3_q8dKS"},"outputs":[{"name":"stdout","output_type":"stream","text":["shapes embeddings (366, 6, 512) (87, 6, 512)\n","shapes y (366, 3) (87, 3)\n","(366, 6, 512)\n","Model: \"sequential_14\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_16 (LSTM)              (None, 3)                 6192      \n","                                                                 \n"," dense_22 (Dense)            (None, 48)                192       \n","                                                                 \n"," dropout_11 (Dropout)        (None, 48)                0         \n","                                                                 \n"," dense_23 (Dense)            (None, 3)                 147       \n","                                                                 \n","=================================================================\n","Total params: 6,531\n","Trainable params: 6,531\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","3\n","[1.03977273 0.96315789]\n","[1.14375    0.88834951]\n","[0.70114943 1.74285714]\n","[[1.03977273 0.96315789]\n"," [1.14375    0.88834951]\n"," [0.70114943 1.74285714]]\n","Epoch 1/80\n","12/12 [==============================] - 3s 42ms/step - loss: 0.6920 - val_loss: 0.6905\n","Epoch 2/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.6893 - val_loss: 0.6879\n","Epoch 3/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6867 - val_loss: 0.6853\n","Epoch 4/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6846 - val_loss: 0.6829\n","Epoch 5/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6819 - val_loss: 0.6803\n","Epoch 6/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6795 - val_loss: 0.6776\n","Epoch 7/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6768 - val_loss: 0.6749\n","Epoch 8/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.6740 - val_loss: 0.6720\n","Epoch 9/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.6712 - val_loss: 0.6690\n","Epoch 10/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.6671 - val_loss: 0.6656\n","Epoch 11/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6649 - val_loss: 0.6624\n","Epoch 12/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.6597 - val_loss: 0.6592\n","Epoch 13/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6561 - val_loss: 0.6560\n","Epoch 14/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6520 - val_loss: 0.6536\n","Epoch 15/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6511 - val_loss: 0.6511\n","Epoch 16/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6483 - val_loss: 0.6489\n","Epoch 17/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6421 - val_loss: 0.6466\n","Epoch 18/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6417 - val_loss: 0.6445\n","Epoch 19/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.6381 - val_loss: 0.6424\n","Epoch 20/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6348 - val_loss: 0.6401\n","Epoch 21/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6323 - val_loss: 0.6381\n","Epoch 22/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6308 - val_loss: 0.6357\n","Epoch 23/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6227 - val_loss: 0.6333\n","Epoch 24/80\n","12/12 [==============================] - 0s 8ms/step - loss: 0.6213 - val_loss: 0.6308\n","Epoch 25/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6168 - val_loss: 0.6279\n","Epoch 26/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6144 - val_loss: 0.6250\n","Epoch 27/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6079 - val_loss: 0.6221\n","Epoch 28/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6030 - val_loss: 0.6189\n","Epoch 29/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.6012 - val_loss: 0.6155\n","Epoch 30/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5949 - val_loss: 0.6121\n","Epoch 31/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5833 - val_loss: 0.6086\n","Epoch 32/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5811 - val_loss: 0.6048\n","Epoch 33/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5750 - val_loss: 0.6010\n","Epoch 34/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5698 - val_loss: 0.5968\n","Epoch 35/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5594 - val_loss: 0.5932\n","Epoch 36/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5571 - val_loss: 0.5895\n","Epoch 37/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.5513 - val_loss: 0.5863\n","Epoch 38/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5413 - val_loss: 0.5830\n","Epoch 39/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5398 - val_loss: 0.5798\n","Epoch 40/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5293 - val_loss: 0.5762\n","Epoch 41/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5277 - val_loss: 0.5740\n","Epoch 42/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.5191 - val_loss: 0.5717\n","Epoch 43/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5139 - val_loss: 0.5702\n","Epoch 44/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.5122 - val_loss: 0.5683\n","Epoch 45/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4998 - val_loss: 0.5665\n","Epoch 46/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4990 - val_loss: 0.5662\n","Epoch 47/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4892 - val_loss: 0.5654\n","Epoch 48/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4898 - val_loss: 0.5651\n","Epoch 49/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4854 - val_loss: 0.5648\n","Epoch 50/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4803 - val_loss: 0.5646\n","Epoch 51/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4778 - val_loss: 0.5649\n","Epoch 52/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4767 - val_loss: 0.5630\n","Epoch 53/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4680 - val_loss: 0.5641\n","Epoch 54/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4722 - val_loss: 0.5646\n","Epoch 55/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4639 - val_loss: 0.5647\n","Epoch 56/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4628 - val_loss: 0.5628\n","Epoch 57/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4582 - val_loss: 0.5624\n","Epoch 58/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4563 - val_loss: 0.5636\n","Epoch 59/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4591 - val_loss: 0.5633\n","Epoch 60/80\n","12/12 [==============================] - 0s 8ms/step - loss: 0.4516 - val_loss: 0.5636\n","Epoch 61/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4436 - val_loss: 0.5645\n","Epoch 62/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4430 - val_loss: 0.5642\n","Epoch 63/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4389 - val_loss: 0.5635\n","Epoch 64/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.4363 - val_loss: 0.5647\n","Epoch 65/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4333 - val_loss: 0.5674\n","Epoch 66/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4276 - val_loss: 0.5656\n","Epoch 67/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4256 - val_loss: 0.5665\n","Epoch 68/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4189 - val_loss: 0.5669\n","Epoch 69/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4171 - val_loss: 0.5689\n","Epoch 70/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4240 - val_loss: 0.5713\n","Epoch 71/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.4104 - val_loss: 0.5670\n","Epoch 72/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.4087 - val_loss: 0.5638\n","Epoch 73/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4077 - val_loss: 0.5664\n","Epoch 74/80\n","12/12 [==============================] - 0s 5ms/step - loss: 0.4059 - val_loss: 0.5712\n","Epoch 75/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.3977 - val_loss: 0.5700\n","Epoch 76/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3957 - val_loss: 0.5696\n","Epoch 77/80\n","12/12 [==============================] - 0s 7ms/step - loss: 0.3975 - val_loss: 0.5731\n","Epoch 78/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3916 - val_loss: 0.5760\n","Epoch 79/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3940 - val_loss: 0.5730\n","Epoch 80/80\n","12/12 [==============================] - 0s 6ms/step - loss: 0.3896 - val_loss: 0.5709\n","accuracy:  0.47126436781609193\n","hamming loss:  0.24521072796934865\n","jaccard score:  0.5986288193438295\n","[[[19 20]\n","  [ 6 42]]\n","\n"," [[23 12]\n","  [10 42]]\n","\n"," [[58  3]\n","  [13 13]]]\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.88      0.76        48\n","           1       0.78      0.81      0.79        52\n","           2       0.81      0.50      0.62        26\n","\n","   micro avg       0.73      0.77      0.75       126\n","   macro avg       0.76      0.73      0.73       126\n","weighted avg       0.75      0.77      0.75       126\n"," samples avg       0.76      0.78      0.74       126\n","\n"]}],"source":["from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","import tensorflow as tf\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score, accuracy_score # we can use gmean\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","for train_index, test_index in msss.split(np.array(X), np.array(y)):\n","\n","  X_train, X_test = X[train_index], X[test_index]\n","  y_train, y_test = y[train_index], y[test_index]\n","\n","  #X_train, X_test = get_embeddings(X_train, X_test, embedding, sequence_size_first)\n","  X_train = vectorize_sentences(X_train, sentence_length)\n","  X_test = vectorize_sentences(X_test, sentence_length)\n","  X_train = np.array(X_train)\n","  X_test = np.array(X_test)\n","  print(\"shapes embeddings\", X_train.shape, X_test.shape)\n","  print(\"shapes y\", y_train.shape, y_test.shape)\n","  print(X_train.shape)\n","  model = lstm(X_train.shape[1:], 3)\n","  print(model.summary())\n","  class_weights = calculating_class_weights(y_train)\n","  #print(class_weights)\n","  opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","  model.compile(optimizer=opt, loss=\"binary_crossentropy\")\n","  model.fit(X_train, tf.cast(y_train, tf.float32), epochs=80, validation_data=(X_test, tf.cast(y_test, tf.float32)))\n","\n","  def to_labels(pos_probs, threshold):\n","          return (pos_probs \u003e= threshold).astype('int')\n","      \n","\n","\n","  y_predict = model.predict(X_test)\n","  from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, hamming_loss, jaccard_score, precision_score, recall_score\n","  thresholds = np.arange(0, 1, 0.005)\n","\n","\n","  #f1_score(y_test, to_labels(y_p, t), average=\"weighted\") * 0.5 + \n","  scores = [f1_score(y_test, to_labels(y_predict, t), average=\"weighted\") for t in thresholds]\n","  ix = np.argmax(scores)\n","  y_predict = y_predict \u003e thresholds[ix]\n","\n","\n","\n","  print(\"accuracy: \", accuracy_score(y_test, y_predict))\n","  print(\"hamming loss: \", hamming_loss(y_test, y_predict))\n","  print(\"jaccard score: \", jaccard_score(y_test, y_predict, average='weighted'))\n","  print(multilabel_confusion_matrix(y_test, y_predict))\n","  print(classification_report(y_test, y_predict))"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1654895448482,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"P1apqV1ELeg4","outputId":"07edc768-4575-4d2f-c345-b2e3ee21bbc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.56      1.00      0.72        48\n","           1       0.60      1.00      0.75        52\n","           2       0.35      0.92      0.51        26\n","\n","   micro avg       0.51      0.98      0.68       126\n","   macro avg       0.50      0.97      0.66       126\n","weighted avg       0.53      0.98      0.69       126\n"," samples avg       0.53      0.99      0.67       126\n","\n","[[[ 1 38]\n","  [ 0 48]]\n","\n"," [[ 0 35]\n","  [ 0 52]]\n","\n"," [[17 44]\n","  [ 2 24]]]\n","0.13793103448275862\n"]}],"source":["from sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score, accuracy_score # we can use gmean\n","\n","\n","print(classification_report(y_test, y_predict))\n","print(multilabel_confusion_matrix(y_test, y_predict)) \n","print(accuracy_score(y_test, y_predict))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5eYtgk6TZiKpcetkB4ng1","name":"DNN.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}