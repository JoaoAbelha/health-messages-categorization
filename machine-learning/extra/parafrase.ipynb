{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","provenance":[],"authorship_tag":"ABX9TyNQbVViHjfAWwvyhshjciiv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nwEGqFbr-3t","executionInfo":{"status":"ok","timestamp":1655822837909,"user_tz":-60,"elapsed":2438,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"b3732e8d-459d-4baf-8d8c-18a3fa51ba87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/gdrive/My Drive/Mestrado/second_level_preprocessed.csv')"],"metadata":{"id":"4cCx7dWhsA6F","executionInfo":{"status":"ok","timestamp":1655822837910,"user_tz":-60,"elapsed":28,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"7HbbPbuusNGN","executionInfo":{"status":"ok","timestamp":1655822837912,"user_tz":-60,"elapsed":27,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"0a4421a1-87f5-4856-e9d0-7d2de4f8236b"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Unnamed: 0  Unnamed: 0.1  \\\n","0             0             0   \n","1             1             1   \n","2             2             2   \n","3             3             3   \n","4             4             4   \n","..          ...           ...   \n","448         448           448   \n","449         449           449   \n","450         450           450   \n","451         451           451   \n","452         452           452   \n","\n","                                                  text  \\\n","0    Hello and hope you are doing well.Understand y...   \n","1    It would be hard to see any significant abnorm...   \n","2    Hi James,  I, too, go to bed by 10:00, then wa...   \n","3    Hi, I am suffering from the same sleep problem...   \n","4        me too, it feels like its morning after 1hour   \n","..                                                 ...   \n","448  I just had a similar experience of feeling a d...   \n","449  Thank you for posting. I saw starting to think...   \n","450  During an induced 4 week coma following a hear...   \n","451  I have been doing his for years. especially at...   \n","452  i have a lot of similar actions im curious to ...   \n","\n","                                                labels  \\\n","0    ['emotional-support', 'informational-support',...   \n","1    ['informational-support', 'sharing-experiences...   \n","2    ['informational-support', 'sharing-experiences...   \n","3                              ['sharing-experiences']   \n","4            ['sharing-experiences', 'esteem-support']   \n","..                                                 ...   \n","448                            ['sharing-experiences']   \n","449                            ['sharing-experiences']   \n","450                            ['sharing-experiences']   \n","451                            ['sharing-experiences']   \n","452                            ['sharing-experiences']   \n","\n","                                        text_processed  \n","0    Hello and hope you are doing well.Understand y...  \n","1    It would be hard to see any significant abnorm...  \n","2    Hi James, I, tsurprised, go to bed by <TIME>, ...  \n","3    Hi, am suffering from the same sleep problems....  \n","4    me tsurprised, it feels like its <TIME> after ...  \n","..                                                 ...  \n","448  just had similar eplayfulerience of feeling dr...  \n","449  Thank you for posting. saw starting to think w...  \n","450  During <DATE> <DISEASE> following heart operat...  \n","451  have been doing his for <DATE>. especially at ...  \n","452  have lot of similar actions am curious to see ...  \n","\n","[453 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-687eb78f-8ef5-40b3-b765-6a6ef7a08839\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>text_processed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Hello and hope you are doing well.Understand y...</td>\n","      <td>['emotional-support', 'informational-support',...</td>\n","      <td>Hello and hope you are doing well.Understand y...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>It would be hard to see any significant abnorm...</td>\n","      <td>['informational-support', 'sharing-experiences...</td>\n","      <td>It would be hard to see any significant abnorm...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Hi James,  I, too, go to bed by 10:00, then wa...</td>\n","      <td>['informational-support', 'sharing-experiences...</td>\n","      <td>Hi James, I, tsurprised, go to bed by &lt;TIME&gt;, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>Hi, I am suffering from the same sleep problem...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>Hi, am suffering from the same sleep problems....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>me too, it feels like its morning after 1hour</td>\n","      <td>['sharing-experiences', 'esteem-support']</td>\n","      <td>me tsurprised, it feels like its &lt;TIME&gt; after ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>448</th>\n","      <td>448</td>\n","      <td>448</td>\n","      <td>I just had a similar experience of feeling a d...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>just had similar eplayfulerience of feeling dr...</td>\n","    </tr>\n","    <tr>\n","      <th>449</th>\n","      <td>449</td>\n","      <td>449</td>\n","      <td>Thank you for posting. I saw starting to think...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>Thank you for posting. saw starting to think w...</td>\n","    </tr>\n","    <tr>\n","      <th>450</th>\n","      <td>450</td>\n","      <td>450</td>\n","      <td>During an induced 4 week coma following a hear...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>During &lt;DATE&gt; &lt;DISEASE&gt; following heart operat...</td>\n","    </tr>\n","    <tr>\n","      <th>451</th>\n","      <td>451</td>\n","      <td>451</td>\n","      <td>I have been doing his for years. especially at...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>have been doing his for &lt;DATE&gt;. especially at ...</td>\n","    </tr>\n","    <tr>\n","      <th>452</th>\n","      <td>452</td>\n","      <td>452</td>\n","      <td>i have a lot of similar actions im curious to ...</td>\n","      <td>['sharing-experiences']</td>\n","      <td>have lot of similar actions am curious to see ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>453 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-687eb78f-8ef5-40b3-b765-6a6ef7a08839')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-687eb78f-8ef5-40b3-b765-6a6ef7a08839 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-687eb78f-8ef5-40b3-b765-6a6ef7a08839');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["def get_tail_label(df):\n","    \"\"\"\n","    Give tail label colums of the given target dataframe\n","    \n","    args\n","    df: pandas.DataFrame, target label df whose tail label has to identified\n","    \n","    return\n","    tail_label: list, a list containing column name of all the tail label\n","    \"\"\"\n","    columns = df.columns\n","    n = len(columns)\n","    irpl = np.zeros(n)\n","    for column in range(n):\n","        irpl[column] = df[columns[column]].value_counts()[1]\n","    irpl = max(irpl)/irpl\n","    mir = np.average(irpl)\n","    tail_label = []\n","    for i in range(n):\n","        if irpl[i] > mir:\n","            tail_label.append(columns[i])\n","    return tail_label\n","\n","def get_index(df):\n","  \"\"\"\n","  give the index of all tail_label rows\n","  args\n","  df: pandas.DataFrame, target label df from which index for tail label has to identified\n","    \n","  return\n","  index: list, a list containing index number of all the tail label\n","  \"\"\"\n","  tail_labels = get_tail_label(df)\n","  index = set()\n","  for tail_label in tail_labels:\n","    sub_index = set(df[df[tail_label]==1].index)\n","    index = index.union(sub_index)\n","  return list(index)\n","\n","def get_minority_instace(X, y):\n","    \"\"\"\n","    Give minority dataframe containing all the tail labels\n","    \n","    args\n","    X: pandas.DataFrame, the feature vector dataframe\n","    y: pandas.DataFrame, the target vector dataframe\n","    \n","    return\n","    X_sub: pandas.DataFrame, the feature vector minority dataframe\n","    y_sub: pandas.DataFrame, the target vector minority dataframe\n","    \"\"\"\n","    index = get_index(y)\n","    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n","    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n","    return X_sub, y_sub\n","\n","def nearest_neighbour(X, neigh=5):\n","    \"\"\"\n","    Give index of 5 nearest neighbor of all the instance\n","    \n","    args\n","    X: np.array, array whose nearest neighbor has to find\n","    \n","    return\n","    indices: list of list, index of 5 NN of each element in X\n","    \"\"\"\n","    nbs=NearestNeighbors(n_neighbors=5,metric='euclidean',algorithm='kd_tree').fit(X)\n","    euclidean,indices= nbs.kneighbors(X)\n","    return indices\n","\n","\n","\n","def MLSMOTE(X, y, n_sample = 100, neigh=5):\n","    \"\"\"\n","    Give the augmented data using MLSMOTE algorithm\n","    \n","    args\n","    X: pandas.DataFrame, input vector DataFrame\n","    y: pandas.DataFrame, feature vector dataframe\n","    n_sample: int, number of newly generated sample\n","    \n","    return\n","    new_X: pandas.DataFrame, augmented feature vector data\n","    target: pandas.DataFrame, augmented target vector data\n","    \"\"\"\n","    indices2 = nearest_neighbour(X, neigh=5)\n","    n = len(indices2)\n","    new_X = np.zeros((n_sample, X.shape[1]))\n","    target = np.zeros((n_sample, y.shape[1]))\n","    for i in range(n_sample):\n","        reference = random.randint(0, n-1)\n","        neighbor = random.choice(indices2[reference, 1:])\n","        all_point = indices2[reference]\n","        nn_df = y[y.index.isin(all_point)]\n","        ser = nn_df.sum(axis = 0, skipna = True)\n","        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n","        ratio = random.random()\n","        gap = X.loc[reference,:] - X.loc[neighbor,:]\n","        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n","    new_X = pd.DataFrame(new_X, columns=X.columns)\n","    target = pd.DataFrame(target, columns=y.columns)\n","    return new_X, target"],"metadata":{"id":"GqhXlnLksD-F","executionInfo":{"status":"ok","timestamp":1655822837913,"user_tz":-60,"elapsed":21,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["X = df['text_processed']\n"],"metadata":{"id":"t94HKrXFsGFk","executionInfo":{"status":"ok","timestamp":1655822837914,"user_tz":-60,"elapsed":21,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["from ast import literal_eval\n","df['labels'] = df['labels'].apply(literal_eval)"],"metadata":{"id":"X_LT9YrAsHdF","executionInfo":{"status":"ok","timestamp":1655822837915,"user_tz":-60,"elapsed":21,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","y=mlb.fit_transform(df['labels'])"],"metadata":{"id":"6suZZALmsKCM","executionInfo":{"status":"ok","timestamp":1655822837915,"user_tz":-60,"elapsed":20,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["! pip install iterative-stratification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68ZpBuTGsXHe","executionInfo":{"status":"ok","timestamp":1655822843166,"user_tz":-60,"elapsed":5269,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"84f6aa0f-afc5-466e-a5cb-79dec191aec5"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.7/dist-packages (0.1.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n"]}]},{"cell_type":"code","source":["! pip install scikit-multilearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sg1rtfFbsYaE","executionInfo":{"status":"ok","timestamp":1655822847151,"user_tz":-60,"elapsed":4006,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"4b6d23a1-2f86-4f8c-91dc-26fd455c3ff2"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"]}]},{"cell_type":"code","source":["def vectorize_sentences(X):\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      current_embedding = embed(sentences).numpy()\n","      new_X.append(current_embedding)\n","    \n","    return np.array(new_X)"],"metadata":{"id":"nFmCHAsWFO_2","executionInfo":{"status":"ok","timestamp":1655828708546,"user_tz":-60,"elapsed":247,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np \n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","#using the count vectorizer\n","count = CountVectorizer()\n","word_count=count.fit_transform(df['text_processed'])\n","\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","tfidf_transformer.fit(word_count)\n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count.get_feature_names(),columns=[\"idf_weights\"])\n","\n","\n","#inverse document frequency\n","df_idf.sort_values(by=['idf_weights'])\n","\n","#tfidf\n","tf_idf_vector=tfidf_transformer.transform(word_count)\n","feature_names = count.get_feature_names()\n","\n","first_document_vector=tf_idf_vector[1]\n","df_tfifd= pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n","\n","df_tfifd.sort_values(by=[\"tfidf\"],ascending=True)\n","\n","temp = df_tfifd.to_dict()\n","\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","stopwords_nltlk = list(set(stopwords.words('english')))\n","\n","ultimate = []\n","for word in stopwords_nltlk:\n","  if word in temp['tfidf'].keys():\n","    if temp['tfidf'][word] == 0 :#and word not in do_not_remove:\n","      ultimate.append(word)\n","\n","#### add ultimate as stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAT3xtgrsb5N","executionInfo":{"status":"ok","timestamp":1655822847153,"user_tz":-60,"elapsed":21,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"702a9737-465a-4e90-ab3b-84b3ce6c7e54"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["import tensorflow_hub as hub\n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"],"metadata":{"id":"gUzb9KVkse-U","executionInfo":{"status":"ok","timestamp":1655822859801,"user_tz":-60,"elapsed":12662,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","import numpy as np\n","\n","def average_sentences(X):\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      current_embedding = embed(sentences).numpy()\n","      '''\n","      diff = number -len(current_embedding)\n","      if diff > 0:\n","          padding = np.zeros((diff, 512))\n","          current_embedding = np.concatenate((padding, current_embedding), axis=0)\n","      '''\n","      new_X.append(np.mean(current_embedding, axis=0))\n","      \n","    \n","    return np.array(new_X)\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSVnN9kTshp1","executionInfo":{"status":"ok","timestamp":1655822859801,"user_tz":-60,"elapsed":21,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"3460c36a-b33e-4910-bdb7-6f48b869ad61"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["########### paraphrase ##############\n","!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UtawFmAzsjqk","executionInfo":{"status":"ok","timestamp":1655822874231,"user_tz":-60,"elapsed":14450,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"4c56f9e8-1c7d-47b6-8ba9-8046d12a96f9"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (\"device \",device)\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntUxiOIMslBk","executionInfo":{"status":"ok","timestamp":1655822914441,"user_tz":-60,"elapsed":40212,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"afb7f5fb-49e7-42ac-e672-f4789a730b69"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["device  cpu\n"]}]},{"cell_type":"code","source":["def paraphrase(context):\n","\n","  text = \"\" + context + \" </s>\"\n","  encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\n","  input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","  model.eval()\n","  diverse_beam_outputs = model.generate(\n","      input_ids=input_ids,attention_mask=attention_mask,\n","      max_length=128,\n","      early_stopping=True,\n","      num_beams=10,\n","      num_beam_groups = 5,\n","      num_return_sequences=2,\n","      diversity_penalty = 0.70\n","  )\n","\n","  ans = []\n","  for beam_output in diverse_beam_outputs:\n","      sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","      ans.append(sent[len(\"'paraphrasedoutput:\") : ])\n","  return ans"],"metadata":{"id":"RBlP-pnvsxjv","executionInfo":{"status":"ok","timestamp":1655822914442,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","import nltk\n","import numpy as np\n","nltk.download('punkt')\n","\n","def paraphrase_sample(text):\n","  n = len(sent_tokenize(text))\n","\n","  result = []\n","  smallest = 1000\n","\n","  for sentence in sent_tokenize(text):\n","    sentence_augments = paraphrase(sentence)\n","\n","   \n","    smallest = min(smallest, len(sentence_augments))\n","    result.append(sentence_augments[0: smallest])\n","\n","\n","  result = np.array(result).T\n","  ans = []\n","  for i in range(len(result)):\n","    ans.append(' '.join(result[i]))\n","    #result[i] = ' '.join(result[i])\n","      \n","  return ans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFoT8ovhszkX","executionInfo":{"status":"ok","timestamp":1655822914442,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"d2516511-4257-4faa-86c3-fbec7495796d"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["def aug(X_train, y_train, index):\n","  \n","  X_new = []\n","  y_new= []\n","\n","  for X, y in zip(X_train[index], y_train[index]):\n","    X_new.append(paraphrase_sample(X))\n","    y_new.append(y)\n","\n","  return X_new, y_new\n"],"metadata":{"id":"BSEh4IEPs1wd","executionInfo":{"status":"ok","timestamp":1655822914443,"user_tz":-60,"elapsed":27,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def do(X_train, X_a):\n"," oop = X_train.tolist()\n"," for a in X_a:\n","   print(a[0])\n","   oop.append(a[0])\n"," return oop\n","\n","def do_y(X_train, X_a):\n"," oop = X_train.tolist()\n"," for a in X_a:\n","   \n","   oop.append(a)\n"," return oop"],"metadata":{"id":"b9k3Utw3s3Du","executionInfo":{"status":"ok","timestamp":1655822914444,"user_tz":-60,"elapsed":26,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["def vectorize_sentences(X):\n","  a = []\n","  for x in X:\n","    print(x)\n","    embeded_tweets = embed(x).numpy()\n","    a.append(embeded_tweets)\n","  return a"],"metadata":{"id":"KveveViAIX51","executionInfo":{"status":"ok","timestamp":1655829572552,"user_tz":-60,"elapsed":259,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.multiclass import OneVsRestClassifier ### does not take into account label correlations\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, hamming_loss, jaccard_score\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","#from sklearn.multioutput import ClassifierChain\n","\n","from sklearn.utils import compute_class_weight\n","from skmultilearn.problem_transform import BinaryRelevance\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","from skmultilearn.problem_transform import ClassifierChain, LabelPowerset\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from skmultilearn.cluster import NetworkXLabelGraphClusterer\n","from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n","from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","\n","# linear is nice 0.47\n","# poly 0.37\n","# rbf 0.34\n","# sigmoid 0.43\n","\n","#from skmultilearn.ext import Keras\n","\n","from skmultilearn.adapt import MLkNN\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","################ run the pipeline #####################\n","\n","#### RF E LR\n","for train_index, test_index in msss.split(np.array(X), np.array(y)):\n","      import warnings\n","      warnings.simplefilter(action='ignore', category=FutureWarning)\n","      ####### performing the different splits here #########\n","      X_train, X_test = X[train_index], X[test_index]\n","      y_train, y_test = y[train_index], y[test_index]\n","\n","      from sklearn.feature_extraction.text import TfidfVectorizer\n","      \n","\n","      X_train = embed(X_train)\n","      print(X_train.shape)\n","\n","      df_y =  pd.DataFrame(y_train, columns=mlb.classes_)\n","      df_X = pd.DataFrame(data = X_train)\n","\n","      X_sub, y_sub = get_minority_instace(df_X, df_y)   \n","      X_res, y_res =MLSMOTE(X_sub, y_sub, X_sub.shape[0])  \n","\n","      X_train = np.concatenate((X_train, X_res.values))\n","      y_train = np.concatenate((y_train, y_res.values))\n","\n","     \n","      #X_train_augmented = pd.concat([df_X, X_res])\n","      #y_train_augmented = pd.concat([df_y, y_res])\n","\n","    \n","      X_test=  embed(X_test)\n","      # X_test = vectorizer.transform(X_test)#average_sentences(X_test) #vectorizer.transform(X_test)\n","      from sklearn.linear_model import LogisticRegression\n","      from sklearn.naive_bayes import GaussianNB\n","\n","      clf = ClassifierChain(GaussianNB(var_smoothing=1e-3))\n","      #clf = BinaryRelevance(SVC(C= 1, kernel=\"linear\", class_weight = \"balanced\"))\n","      clf.fit(X_train, y_train)\n","\n","      y_pred = clf.predict(X_test)\n","      \n","      from sklearn.metrics import classification_report, jaccard_score, hamming_loss\n","      print(classification_report(y_test, y_pred))\n","      print(jaccard_score(y_test, y_pred, average= \"weighted\"))\n","      print(hamming_loss(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9yeKPS9s6DV","executionInfo":{"status":"ok","timestamp":1655835738850,"user_tz":-60,"elapsed":37878,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"3a662c8a-4c17-451f-b392-7ddd613fa1c0"},"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["(361, 512)\n","              precision    recall  f1-score   support\n","\n","           0       0.25      0.07      0.11        15\n","           1       0.00      0.00      0.00         7\n","           2       0.77      0.55      0.64        42\n","           3       0.40      0.10      0.16        20\n","           4       0.00      0.00      0.00         7\n","           5       1.00      0.27      0.42        26\n","           6       0.72      0.45      0.55        40\n","\n","   micro avg       0.72      0.32      0.45       157\n","   macro avg       0.45      0.20      0.27       157\n","weighted avg       0.63      0.32      0.41       157\n"," samples avg       0.48      0.37      0.39       157\n","\n","0.28411400266606285\n","0.1956521739130435\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["X_res.values.shape, X_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRUswahSTsjK","executionInfo":{"status":"ok","timestamp":1655835140297,"user_tz":-60,"elapsed":270,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"941f753b-eb9b-4f93-9fe0-27a849f73352"},"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((30, 512), TensorShape([361, 512]))"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["np.concatenate((X_res, X_train)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXJ37bcdd2af","executionInfo":{"status":"ok","timestamp":1655835195143,"user_tz":-60,"elapsed":306,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"d6824893-f8f5-4fb2-8b4e-363c47081216"},"execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(391, 512)"]},"metadata":{},"execution_count":129}]}]}