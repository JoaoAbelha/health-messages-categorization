{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KoEHHkR6E-9z","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"error","timestamp":1655801242850,"user_tz":-60,"elapsed":5699,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"outputId":"6732ad28-51a5-48f9-9f23-70dcbec12347"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b79d998bf33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKIe2k8TFMH3","executionInfo":{"status":"aborted","timestamp":1655801242825,"user_tz":-60,"elapsed":30,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('/content/gdrive/My Drive/Mestrado/second_level_preprocessed.csv')\n","#df = pd.read_csv('/content/gdrive/My Drive/Mestrado/pre_processing_important.csv')"]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"JSmRCRINjBtX","executionInfo":{"status":"aborted","timestamp":1655801242826,"user_tz":-60,"elapsed":31,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3keD3LJy9v6M","executionInfo":{"status":"aborted","timestamp":1655801242827,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["# extra preprocessing\n","\n","df['text_processed'] = df['text_processed'].str.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SSkUity17Z9","executionInfo":{"status":"aborted","timestamp":1655801242827,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","#using the count vectorizer\n","count = CountVectorizer()\n","word_count=count.fit_transform(df['text_processed'])\n","\n","tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","tfidf_transformer.fit(word_count)\n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count.get_feature_names(),columns=[\"idf_weights\"])\n","\n","\n","#inverse document frequency\n","df_idf.sort_values(by=['idf_weights'])\n","\n","#tfidf\n","tf_idf_vector=tfidf_transformer.transform(word_count)\n","feature_names = count.get_feature_names()\n","\n","first_document_vector=tf_idf_vector[1]\n","df_tfifd= pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n","\n","df_tfifd.sort_values(by=[\"tfidf\"],ascending=True)"]},{"cell_type":"code","source":["do_not_remove = {\"no\", \"nor\", \"not\", \"few\", \"very\",  \"don'\", \"don't\",  \"'aren'\",\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"}"],"metadata":{"id":"wZUMfogy7Apq","executionInfo":{"status":"aborted","timestamp":1655801242828,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GR3wjkHe19Gh","executionInfo":{"status":"aborted","timestamp":1655801242828,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["temp = df_tfifd.to_dict()\n","\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","stopwords_nltlk = list(set(stopwords.words('english')))\n","\n","ultimate = []\n","for word in stopwords_nltlk:\n","  if word in temp['tfidf'].keys():\n","    if temp['tfidf'][word] == 0 and word not in do_not_remove:\n","      ultimate.append(word)\n","\n","#### add ultimate as stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HmOeooi9WAX","executionInfo":{"status":"aborted","timestamp":1655801242828,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5BbAc_x8m3y","executionInfo":{"status":"aborted","timestamp":1655801242829,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df['text'][111]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuhRb6aJ9Yt4","executionInfo":{"status":"aborted","timestamp":1655801242829,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leNlMStI789A","executionInfo":{"status":"aborted","timestamp":1655801242829,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ra57maIAwArb","executionInfo":{"status":"aborted","timestamp":1655801242829,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLaMQJRIaofh","executionInfo":{"status":"aborted","timestamp":1655801242830,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import string\n","regular_punct = list(string.punctuation)\n","def remove_punctuation(text,punct_list):\n","    for punc in punct_list:\n","        if punc in text:\n","            text = text.replace(punc, ' ' )\n","    return text.strip()\n","\n","df['punct'] = df['text_processed'].apply(lambda x : remove_punctuation(x, regular_punct)).apply(lambda x : \" \".join(x.split()))\n","\n","\n","\n","from nltk.stem.porter import PorterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","\n","def stem_sentences(sentence):\n","    tokens = sentence.split()\n","    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n","    return ' '.join(stemmed_tokens)\n","\n","df['porter_original'] = df['text_processed'].apply(stem_sentences)\n","df['porter_punct'] = df['punct'].apply(stem_sentences)\n","\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","#example text text = 'What can I say about this place. The staff of these restaurants is nice and the eggplant is not bad'\n","\n","class Splitter(object):\n","    \"\"\"\n","    split the document into sentences and tokenize each sentence\n","    \"\"\"\n","    def __init__(self):\n","        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n","        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n","\n","    def split(self,text):\n","        \"\"\"\n","        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n","        \"\"\"\n","        # split into single sentence\n","        sentences = self.splitter.tokenize(text)\n","        # tokenization in each sentences\n","        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n","        return tokens\n","\n","\n","class LemmatizationWithPOSTagger(object):\n","    def __init__(self):\n","        pass\n","    def get_wordnet_pos(self,treebank_tag):\n","        \"\"\"\n","        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n","        \"\"\"\n","        if treebank_tag.startswith('J'):\n","            return wordnet.ADJ\n","        elif treebank_tag.startswith('V'):\n","            return wordnet.VERB\n","        elif treebank_tag.startswith('N'):\n","            return wordnet.NOUN\n","        elif treebank_tag.startswith('R'):\n","            return wordnet.ADV\n","        else:\n","            # As default pos in lemmatization is Noun\n","            return wordnet.NOUN\n","\n","    def pos_tag(self,tokens):\n","        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n","        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n","\n","        # lemmatization using pos tagg   \n","        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n","        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n","        return pos_tokens\n","\n","lemmatizer = WordNetLemmatizer()\n","splitter = Splitter()\n","lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n","\n","#step 1 split document into sentence followed by tokenization\n","\n","def lemmatise(text):\n","  tokens = splitter.split(text)\n","  #step 2 lemmatization using pos tagger \n","  lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n","  ans = \"\"\n","  for sentence in lemma_pos_token:\n","    for word in sentence:\n","      ans += word[1] + \" \"\n","\n","  return ans[:-1]\n","\n","\n","df['lemma_original'] = df['text_processed'].apply(lemmatise)\n","df['lemma_punct'] = df['text_processed'].apply(lemmatise)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDMwrNuP8Smi","executionInfo":{"status":"aborted","timestamp":1655801242830,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGf8rlDCjhYP","executionInfo":{"status":"aborted","timestamp":1655801242830,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["mappings = {\n","    '0': 'information_support',\n","    '1': 'information_support',\n","    '2': 'information_support',\n","\n","    '3':  'emotional_support',\n","    '4': 'emotional_support',\n","    '5': 'emotional_support',\n","    '6': 'emotional_support',\n","    '7': 'emotional_support',\n","\n","    '8' : 'esteem_support',\n","    '9': 'esteem_support',\n","    \n","    '10': 'network_support',\n","    '11': 'network_support',\n","\n","    #'12': 'perform_direct_task',\n","    #'13': 'express_willingness',\n","\n","    '14': 'seeking_support',\n","    '15': 'seeking_support',\n","\n","    '16': 'gratitude',\n","    '17': 'congratulations',\n","    '18' : 'sharing_personal_experiences' \n","\n","}\n","\n","\n","mappings_first = {\n","    '0': 'offering_support',\n","    '1': 'offering_support',\n","    '2': 'offering_support',\n","\n","    '3':  'offering_support',\n","    '4': 'offering_support',\n","    '5': 'offering_support',\n","    '6': 'offering_support',\n","    '7': 'offering_support',\n","\n","    '8' : 'offering_support',\n","    '9': 'offering_support',\n","    \n","    '10': 'offering_support',\n","    '11': 'offering_support',\n","\n","    '12': 'offering_support',\n","    '13': 'offering_support',\n","\n","    '14': 'seeking_support',\n","    '15': 'seeking_support',\n","\n","    '16': 'group_interactions',\n","    '17': 'group_interactions',\n","    '18' : 'group_interactions' \n","}\n","\n","\n","def do(labels):\n","  labels = labels.split(',')\n","  new_labels = []\n","  for label in labels:\n","    if label in mappings_first:\n","      new_labels.append(mappings_first[label])\n","  return list(set(new_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFBjJRXkjUAy","executionInfo":{"status":"aborted","timestamp":1655801242830,"user_tz":-60,"elapsed":32,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#df.labels = df.labels.apply(do)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5lXbLy2uRyj","executionInfo":{"status":"aborted","timestamp":1655801242831,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df = df[df['labels'].apply(len) > 0]\n","### this is required due to the stratified algorithm split used\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhJQaiP1hAzj","executionInfo":{"status":"aborted","timestamp":1655801242831,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxKc_hCjtp07","executionInfo":{"status":"aborted","timestamp":1655801242831,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#df['labels'] = df['labels'].apply(do)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EO_3kuCv0wM","executionInfo":{"status":"aborted","timestamp":1655801242831,"user_tz":-60,"elapsed":33,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["type(df['labels'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9jjeTNitduM","executionInfo":{"status":"aborted","timestamp":1655801242832,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#from ast import literal_eval\n","#df['labels'] = df['labels'].apply(literal_eval)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ti0HJoY5FkI-","executionInfo":{"status":"aborted","timestamp":1655801242832,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["! pip install scikit-multilearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wBduPICLS53","executionInfo":{"status":"aborted","timestamp":1655801242832,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["! pip install iterative-stratification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDPndXlHKCK9","executionInfo":{"status":"aborted","timestamp":1655801242833,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["\n","from sklearn.neighbors import NearestNeighbors\n","import random\n","\n","\n","def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n","    \"\"\"\n","    return\n","    X_sub: pandas.DataFrame, the feature vector minority dataframe\n","    y_sub: pandas.DataFrame, the target vector minority dataframe\n","    \"\"\"\n","\n","    def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n","      \"\"\"\n","      Find the underrepresented targets.\n","      Underrepresented targets are those which are observed less than the median occurance.\n","      Targets beyond a quantile limit are filtered.\n","      \"\"\"\n","      irlbl = df.sum(axis=0)\n","      irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n","      irlbl = irlbl.max() / irlbl\n","      threshold_irlbl = irlbl.median()\n","      tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n","      return tail_label\n","\n","    \n","    tail_labels = get_tail_label(y, ql=ql)\n","    print(set(tail_labels))\n","    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n","    \n","    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n","    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n","    return X_sub, y_sub\n","\n","\n","def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n","    \"\"\"\n","    Give index of 10 nearest neighbor of all the instance\n","    \n","    args\n","    X: np.array, array whose nearest neighbor has to find\n","    \n","    return\n","    indices: list of list, index of 5 NN of each element in X\n","    \"\"\"\n","    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n","    euclidean, indices = nbs.kneighbors(X)\n","    return indices\n","\n","\n","def MLSMOTE(X, y, n_sample = 100, neigh=5):\n","    \"\"\"\n","    Give the augmented data using MLSMOTE algorithm\n","    \n","    args\n","    X: pandas.DataFrame, input vector DataFrame\n","    y: pandas.DataFrame, feature vector dataframe\n","    n_sample: int, number of newly generated sample\n","    \n","    return\n","    new_X: pandas.DataFrame, augmented feature vector data\n","    target: pandas.DataFrame, augmented target vector data\n","    \"\"\"\n","    indices2 = nearest_neighbour(X, neigh=5)\n","    n = len(indices2)\n","    new_X = np.zeros((n_sample, X.shape[1]))\n","    target = np.zeros((n_sample, y.shape[1]))\n","    for i in range(n_sample):\n","        reference = random.randint(0, n-1)\n","        neighbor = random.choice(indices2[reference, 1:])\n","        all_point = indices2[reference]\n","        nn_df = y[y.index.isin(all_point)]\n","        ser = nn_df.sum(axis = 0, skipna = True)\n","        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n","        ratio = random.random()\n","        gap = X.loc[reference,:] - X.loc[neighbor,:]\n","        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n","    new_X = pd.DataFrame(new_X, columns=X.columns)\n","    target = pd.DataFrame(target, columns=y.columns)\n","    return new_X, target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga6blAwG2lyp","executionInfo":{"status":"aborted","timestamp":1655801242833,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from sklearn.utils import class_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUDnJ5XEypsy","executionInfo":{"status":"aborted","timestamp":1655801242833,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, BatchNormalization, Input\n","import tensorflow as tf\n","\n","\n","def create_model_multiclass(input_dim, output_dim):\n","\n","    inp = Input(input_dim)\n","    x = Dense(16, activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros')(inp)\n","    x = Dropout(0.3)(x, training=True)\n","    out = Dense(output_dim, activation='softmax',kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros')(x)\n","    model = Model(inputs = inp, outputs = out)\n","    \n","    opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","\n","    #optimizer\n","    '''\n","    opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","    # create model\n","    model = Sequential()\n","    model.add(Dense(16, input_dim=input_dim, activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    model.add(Dropout(0.3))\n","    #model.add(BatchNormalization())\n","    model.add(Dense(output_dim, activation='softmax',kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    '''\n","\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","\n","def create_model_single_class(input_dim, output_dim):\n","    \n","    # create model\n","    model = Sequential()\n","    model.add(Dense(12, input_dim=input_dim, activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(8, activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    model.add(Dense(output_dim, activation='sigmoid'))\n","    # Compile model\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n","\n","def create_model_multi_label():\n","    #optimizer\n","    opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n","    # create model\n","    model = Sequential()\n","    model.add(Dense(16, input_dim=(None, ), activation='relu', kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    model.add(Dropout(0.3))\n","    model.add(Dense(6, activation='softmax',kernel_initializer= tf.keras.initializers.HeNormal(), bias_initializer='zeros'))\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIbQ6B220F06","executionInfo":{"status":"aborted","timestamp":1655801242833,"user_tz":-60,"elapsed":34,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip install arff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJ6QJvpEQZHM","executionInfo":{"status":"aborted","timestamp":1655801242834,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from sklearn.base import TransformerMixin, BaseEstimator\n","\n","class Debug(BaseEstimator, TransformerMixin):\n","\n","    def transform(self, X):\n","        print(\"Shape of Pre-processed Data:\", X.shape)\n","        print(type(X))\n","        return X\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","from scipy import sparse\n","class SparserTransformer(TransformerMixin):\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def transform(self, X, y=None, **fit_params):\n","        return sparse.csr_matrix(X)\n","\n","class DenseTransformer(TransformerMixin):\n","\n","    def fit(self, X, y=None, **fit_params):\n","        return self\n","\n","    def transform(self, X, y=None, **fit_params):\n","        return X.todense()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTUQQWNJl2rl","executionInfo":{"status":"aborted","timestamp":1655801242834,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip uninstall scikit-learn -y\n","!pip install scikit-learn==0.24.1 # in order to use multisklearn classifiers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUp1HjXB6qmw","executionInfo":{"status":"aborted","timestamp":1655801242834,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#### to use label network embeddings\n","!pip install networkx\n","!git clone https://github.com/thunlp/OpenNE/\n","!pip install -e OpenNE/src"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRnr-3J02bgD","executionInfo":{"status":"aborted","timestamp":1655801242834,"user_tz":-60,"elapsed":35,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#!pip install textattack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro92i8hx3OGz","executionInfo":{"status":"aborted","timestamp":1655801242835,"user_tz":-60,"elapsed":36,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#! pip install tensorflow_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXK8weAo9tRw","executionInfo":{"status":"aborted","timestamp":1655801242835,"user_tz":-60,"elapsed":36,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#from textattack.augmentation import CLAREAugmenter\n","#clare_aug = CLAREAugmenter(pct_words_to_swap=0.2, transformations_per_example=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7znvtk5ZrXz","executionInfo":{"status":"aborted","timestamp":1655801242836,"user_tz":-60,"elapsed":36,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["%%time\n","#clare_aug.augment(\"This is simply a simple text\")\n","#clare_aug.augment(\"This is simply a simple text\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzhcr-Ceji-g","executionInfo":{"status":"aborted","timestamp":1655801242836,"user_tz":-60,"elapsed":36,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["text = \"This is simply a test. And thi is another phrase\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkBlrxmuccAo","executionInfo":{"status":"aborted","timestamp":1655801242836,"user_tz":-60,"elapsed":36,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["%%time\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","\n","\n","def augment_sample(text, augmenter, number):\n","  n = len(sent_tokenize(text))\n","\n","  result = [''] * number\n","\n","\n","  for sentence in sent_tokenize(text):\n","    sentence_augments = augmenter.augment(sentence)\n","    if len(sentence_augments) < number:\n","      diff = number - len(sentence_augments)\n","      sentence_augments.extend([sentence] * diff)\n","\n","    for i, (r, s) in enumerate(zip(result, sentence_augments)):\n","      result[i] = r + s \n","\n","  return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBR5Nnj-gXTA","executionInfo":{"status":"aborted","timestamp":1655801242837,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import nltk\n","from nltk.tag import pos_tag\n","from nltk import sent_tokenize\n","from nltk.corpus import wordnet\n","\n","def get_synonym_for_word(word):\n","    \"\"\"returns the synonym given word if found, otherwise returns the same word\"\"\"\n","    \n","    synonyms = []\n","    for syn in wordnet.synsets(word):\n","\n","        for l in syn.lemmas():\n","            synonyms.append(l.name())\n","    synonyms = [syn for syn in synonyms if syn!=word]\n","    if len(synonyms) == 0:\n","        return word\n","    else:\n","        return synonyms[0]\n","\n","\n","\n","def augment_sentence_wordnet(sentence, filters=['NN', 'JJ']):\n","    \"\"\"Augments words in sentence which are filtered by pos tags\"\"\"\n","    \n","    pos_sent = pos_tag(sentence.split())\n","    new_sent = []\n","    for word,tag in pos_sent:\n","        if tag in filters:\n","            new_sent.append(get_synonym_for_word(word))\n","        else:\n","            new_sent.append(word)\n","            \n","    return \" \".join(new_sent)\n","\n","\n","def augment_data(data, target):\n","    \"\"\"Creates augmented data using wordnet synonym imputation.\"\"\"\n","    \n","    aug_data = []\n","    aug_target = []\n","    for row, t in zip(data, target):\n","        aug_row = []\n","        row_sents = sent_tokenize(row)\n","        #print(\"row_sents\", row_sents)\n","        for line in row_sents:\n","            line = augment_sentence_wordnet(line)\n","            aug_row.append(line)\n","        row_sents = \" \".join(aug_row)\n","        \n","        #print(row_sents)\n","        aug_data.append(row)\n","        aug_data.append(row_sents)\n","        aug_target.append(t)\n","        aug_target.append(t)\n","        #print(len(aug_data))\n","    return aug_data, aug_target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrcFc2sN2L5j","executionInfo":{"status":"aborted","timestamp":1655801242837,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["##################### This part is meant to perform some data augmentation ################\n","\n","\n","def textattack_data_augment(data, target, texattack_augmenter, number):\n","\n","  '''\n","    generic function to perform data augmentation\n","    This needs to receive the textual data rather than the vectorized one\n","  '''\n","\n","  aug_data = []\n","  aug_label = []\n","\n","  for text, label in zip(data, target):\n","    \n","    #### random number [0,2] and perform data augmentation with 1/3 probability\n","    if random.randint(0,2) != 1:\n","      aug_data.append(text)\n","      aug_label.append(label)\n","      continue\n","\n","    ### perform data augmentation of that label instance\n","    #aug_list = texattack_augmenter.augment(text)\n","    aug_list = augment_sample(text, texattack_augmenter, number)\n","    aug_data.append(text)\n","    aug_label.append(label)\n","    aug_data.extend(aug_list)\n","    aug_label.extend([label]*len(aug_list))\n","\n","  return aug_data, aug_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdBfq9UvJ-nq","executionInfo":{"status":"aborted","timestamp":1655801242837,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["########### paraphrase ##############\n","!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QGDSmt0U_wZ","executionInfo":{"status":"aborted","timestamp":1655801242838,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","#model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","#tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n","#import torch\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#print (\"device \",device)\n","#model = model.to(device)\n","# Diverse Beam search\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkA1yHuhM-GF","executionInfo":{"status":"aborted","timestamp":1655801242838,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["def paraphrase(context):\n","\n","  text = \"\" + context + \" </s>\"\n","  encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\n","  input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","  model.eval()\n","  diverse_beam_outputs = model.generate(\n","      input_ids=input_ids,attention_mask=attention_mask,\n","      max_length=128,\n","      early_stopping=True,\n","      num_beams=10,\n","      num_beam_groups = 5,\n","      num_return_sequences=2,\n","      diversity_penalty = 0.70\n","  )\n","\n","  ans = []\n","  for beam_output in diverse_beam_outputs:\n","      sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","      ans.append(sent[len(\"'paraphrasedoutput:\") : ])\n","  return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WNK9qyaaqYa","executionInfo":{"status":"aborted","timestamp":1655801242838,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","import nltk\n","import numpy as np\n","nltk.download('punkt')\n","\n","def paraphrase_sample(text):\n","  n = len(sent_tokenize(text))\n","\n","  result = []\n","  smallest = 1000\n","\n","  for sentence in sent_tokenize(text):\n","    sentence_augments = paraphrase(sentence)\n","\n","   \n","    smallest = min(smallest, len(sentence_augments))\n","    result.append(sentence_augments[0: smallest])\n","\n","\n","  result = np.array(result).T\n","  ans = []\n","  for i in range(len(result)):\n","    ans.append(' '.join(result[i]))\n","    #result[i] = ' '.join(result[i])\n","      \n","  return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3uLur1SYudg","executionInfo":{"status":"aborted","timestamp":1655801242838,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["##################### This part is meant to perform some data augmentation ################\n","\n","\n","def paraphrase_augmentation(data, target):\n","\n","  '''\n","    generic function to perform data augmentation\n","    This needs to receive the textual data rather than the vectorized one\n","  '''\n","\n","  aug_data = []\n","  aug_label = []\n","\n","  for text, label in zip(data, target):\n","    \n","    #### random number [0,2] and perform data augmentation with 1/3 probability\n","    if random.randint(0,2) != 1:\n","      aug_data.append(text)\n","      aug_label.append(label)\n","      continue\n","\n","    ### perform data augmentation of that label instance\n","    #aug_list = texattack_augmenter.augment(text)\n","    aug_list = paraphrase_sample(text)\n","    aug_data.append(text)\n","    aug_label.append(label)\n","    aug_data.extend(aug_list)\n","    aug_label.extend([label]*len(aug_list))\n","\n","  return aug_data, aug_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zvo922vrunB","executionInfo":{"status":"aborted","timestamp":1655801242839,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip install textstat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB6ufTdtwTzp","executionInfo":{"status":"aborted","timestamp":1655801242839,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df['text'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxndRzFMCw2v","executionInfo":{"status":"aborted","timestamp":1655801242839,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from nltk import word_tokenize\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiO8ISE2-UPq","executionInfo":{"status":"aborted","timestamp":1655801242839,"user_tz":-60,"elapsed":37,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfDaAszL12Zp","executionInfo":{"status":"aborted","timestamp":1655801242841,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import gensim\n","from gensim.models import word2vec\n","from gensim.models import KeyedVectors\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmXyb2khAbT9","executionInfo":{"status":"aborted","timestamp":1655801242841,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import gensim.downloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJNT8A1UScnI","executionInfo":{"status":"aborted","timestamp":1655801242841,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["def get_embedding_result(word, embedding):\n","  return embedding[word] if word in embedding else 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clr_oOLT3mfL","executionInfo":{"status":"aborted","timestamp":1655801242841,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["def get_gloVe(X_train, X_test, embeddings_index):\n","  X_train_vector = []\n","  for text in X_train:\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_train_vector.append(np.average(words, axis = 0))\n","\n","  X_test_vector = []\n","  for text in X_test:\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_test_vector.append(np.average(words, axis = 0))\n","\n","    \n","  return X_train_vector, X_test_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TxvIpADy4OB","executionInfo":{"status":"aborted","timestamp":1655801242842,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def get_gloVe_tf_idf(X_train, X_test, embeddings_index):\n","  tfidf = TfidfVectorizer()\n","  tfidf.fit(X_train)\n","  idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n","\n","  train_vector = []\n","  for text in X_train:\n","    weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    train_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  test_vector = []\n","  for text in X_test:\n","    weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    test_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  return train_vector, test_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEWmsjBUdOYm","executionInfo":{"status":"aborted","timestamp":1655801242842,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import tensorflow_hub as hub\n","\n","embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"cUXjRneNd1Fm","executionInfo":{"status":"error","timestamp":1655829527139,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}},"colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"e58767ae-bea0-4830-c460-fa1dc30f1786"},"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1b650edbab90>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    for x in X:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"source":["def vectorize_sentences(X):\n","  a = []\n","    for x in X:\n","      embeded_tweets = embed(x).numpy()\n","      a.append(embeded_tweets)\n","  return a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiFQeTYNqf4l","executionInfo":{"status":"aborted","timestamp":1655801242842,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["############### feature engineering ##################\n","\n","\n","# a) text metadata length of messages and count frequency words\n","### some of them should be applied to the original text\n","\n","def count_chars(text):\n","    return len(text)\n","\n","\n","def count_words(text):\n","  return len(text.split())\n","\n","def count_capital_words(text):\n","    return sum(map(str.isupper,text.split()))\n"," \n","\n","def count_capital_chars(text):\n","    count=0\n","    for i in text:\n","        if i.isupper():\n","            count+=1\n","    return count\n","\n","def count_punctuations(text):\n","  punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n","  counter = 0\n","  for i in punctuations:\n","      counter += 1\n","  return counter\n","\n","def count_sent(text):\n","    return len(nltk.sent_tokenize(text))\n","\n","def count_unique_words(text):\n","    return len(set(text.split()))\n","\n","\n","def get_length_features(X):\n","\n","  features = []\n","  for instance in X:\n","    char_count = count_chars(instance)\n","    word_count = count_words(instance)\n","    sentence_count = count_sent(instance)\n","    unique_words = count_unique_words(instance)\n","\n","    features.append([char_count, word_count, sentence_count, count_capital_chars(instance), count_capital_words(instance), unique_words,\\\n","                     count_punctuations(instance), char_count/word_count, word_count/sentence_count, unique_words/word_count])\n","    \n","  return features\n","\n","#df['avg_wordlength'] = df['char_count']/df['word_count']\n","#df['avg_sentlength'] = df['word_count']/df['sent_count']\n","#df['unique_vs_words'] = df['unique_word_count']/df['word_count']\n","\n","\n","##### sentiment score with vader and textblob #######\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from textblob import TextBlob\n","nltk.download('vader_lexicon')\n","\n","def calc_sentiment_scores(X):\n","    sid = SentimentIntensityAnalyzer()\n","    neg = []\n","    neu = []\n","    pos = []\n","    compound = []\n","\n","    sentiment_textblob = []\n","    subjectivity = []\n","\n","    for text in X:\n","        sentiments = sid.polarity_scores(text)\n","        neg.append(sentiments['neg'])\n","        neu.append(sentiments['neu'])\n","        pos.append(sentiments['pos'])\n","        compound.append(sentiments['compound'])\n","        res = TextBlob(text).sentiment\n","        sentiment_textblob.append(res[0])\n","        subjectivity.append(res[1])\n","\n","    neg = np.array(neg).reshape(-1,1)\n","    neu = np.array(neu).reshape(-1,1)\n","    pos = np.array(pos).reshape(-1,1)\n","    compound = np.array(compound).reshape(-1,1)\n","    sentiment_textblob = np.array(sentiment_textblob).reshape(-1, 1)\n","    subjectivity = np.array(subjectivity).reshape(-1, 1)\n","\n","    return np.concatenate([neg,  pos, compound, sentiment_textblob, subjectivity], axis = 1)\n","\n","\n","####### NER feature engineering #########\n","\n","import collections\n","\n","def utils_lst_count(lst):\n","    dic_counter = collections.Counter()\n","    for x in lst:\n","        dic_counter[x] += 1\n","    dic_counter = collections.OrderedDict( \n","                     sorted(dic_counter.items(), \n","                     key=lambda x: x[1], reverse=True))\n","    lst_count = [ {key:value} for key,value in dic_counter.items() ]\n","    return lst_count\n","\n","# https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n","def utils_ner_features(lst_dics_tuples, tag):\n","    if len(lst_dics_tuples) > 0:\n","        tag_type = []\n","        dic_counter = collections.Counter()\n","        for tuple in lst_dics_tuples:\n","              dic_counter[tuple[1]]+=1\n","        return dic_counter[tag]\n","    else:\n","        return 0\n","\n","## extract ner features\n","\n","def append_ner_features(df, col_tag):\n","  tags_set = []\n","  for lst in df[col_tag].tolist():\n","      for dic in lst:\n","            tags_set.append(dic[1])\n","            #for k in dic.keys():\n","                #tags_set.append(k[1])\n","  tags_set = list(set(tags_set))\n","  for feature in tags_set:\n","      val = df[col_tag].apply(lambda x:   utils_ner_features(x, feature))\n","      print(feature, val)\n","      \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp4b-NjnAUHu","executionInfo":{"status":"aborted","timestamp":1655801242843,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import spacy.cli\n","spacy.cli.download(\"en_core_web_lg\")\n","## call model\n","ner = spacy.load(\"en_core_web_lg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jj5hgV-AEyQc","executionInfo":{"status":"aborted","timestamp":1655801242843,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["def utils_ner_features(lst_dics_tuples, tag):\n","    if len(lst_dics_tuples) > 0:\n","        tag_type = []\n","        dic_counter = collections.Counter()\n","        for tuple in lst_dics_tuples:\n","              dic_counter[tuple[1]]+=1\n","        return dic_counter[tag]\n","    else:\n","        return 0\n","\n","\n","def append_ner_features(df, col_tag, tags_set):\n"," \n","  for feature in tags_set:\n","      df[feature] = df[col_tag].apply(lambda x: \n","                                                    utils_ner_features(x, feature))\n","      \n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CbqQGe1jed6","executionInfo":{"status":"aborted","timestamp":1655801242843,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df2 = pd.DataFrame()\n","df2['tags'] = df['text'].apply(lambda x: [ (tag.text, tag.label_) for tag in ner(x).ents] )\n","tags_set = []\n","for lst in df2['tags'].tolist():\n","    for dic in lst:\n","          tags_set.append(dic[1])\n","\n","tags_set = list(set(tags_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JufMa-S1I7oz","executionInfo":{"status":"aborted","timestamp":1655801242843,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from collections import defaultdict\n","nltk.download('averaged_perceptron_tagger')\n","def PoS_counter(X):\n","\n","  adjectives = []\n","  nouns = []\n","  adverbs = []\n","  verb = []\n","  for text in X:\n","    text = word_tokenize(text)\n","    counter = defaultdict(int)\n","    for word in  nltk.pos_tag(text):\n","      if word[1].startswith('J'):\n","        counter['adjective'] += 1\n","      elif word[1].startswith('N'):\n","        counter['noun'] += 1\n","      elif word[1].startswith('RB'):\n","        counter['adverb'] += 1\n","      elif word[1].startswith('VB'):\n","        counter['verb'] += 1\n","\n","    adjectives.append(counter['adjectives'])\n","    nouns.append(counter['noun'])\n","    adverbs.append(counter['adverbs'])\n","    verb.append(counter['verb'])\n","\n","  adjectives = np.array(adjectives).reshape(-1, 1)\n","  nouns = np.array(nouns).reshape(-1, 1)\n","  adverbs = np.array(adverbs).reshape(-1, 1)\n","  verb = np.array(verb).reshape(-1, 1)\n","\n","\n","  return np.concatenate([adjectives, nouns, adverbs, verb ], axis = 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MOs6cwxqH05","executionInfo":{"status":"aborted","timestamp":1655801242844,"user_tz":-60,"elapsed":40,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip install pyLDAvis -qq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plGhahE1p-d8","executionInfo":{"status":"aborted","timestamp":1655801242844,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["################# topic modeling #################\n","\n","# Import\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","import spacy\n","import pyLDAvis.gensim_models\n","pyLDAvis.enable_notebook()# Visualise inside a notebook\n","from gensim.corpora.dictionary import Dictionary\n","from gensim.models import LdaMulticore\n","from gensim.models import CoherenceModel\n","\n","# Read the data\n","reports = df\n","reports.head()\n","reports.info()\n","\n","# Our spaCy model:\n","nlp = ner\n","\n","# Tags I want to remove from the text\n","removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n","tokens = []\n","\n","for summary in nlp.pipe(reports['text']):\n","   proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n","   tokens.append(proj_tok)\n","\n","# Add tokens to new column\n","reports['tokens'] = tokens\n","reports['tokens']\n","\n","# Create dictionary\n","# I will apply the Dictionary Object from Gensim, which maps each word to their unique ID:\n","dictionary = Dictionary(reports['tokens'])\n","print(dictionary.token2id)\n","\n","# Filter dictionary\n","dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n","\n","# Create corpus\n","corpus = [dictionary.doc2bow(doc) for doc in reports['tokens']]\n","\n","# LDA model building\n","lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)\n","\n","# Coherence score using C_umass:\n","topics = []\n","score = []\n","for i in range(1,20,1):\n","   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n","   cm = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n","   topics.append(i)\n","   score.append(cm.get_coherence())\n","_=plt.plot(topics, score)\n","_=plt.xlabel('Number of Topics')\n","_=plt.ylabel('Coherence Score')\n","plt.show()\n","\n","# Coherence score using C_v:\n","topics = []\n","score = []\n","for i in range(1,20,1):\n","   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n","   cm = CoherenceModel(model=lda_model, texts = reports['tokens'], corpus=corpus, dictionary=dictionary, coherence='c_v')\n","   topics.append(i)\n","   score.append(cm.get_coherence())\n","_=plt.plot(topics, score)\n","_=plt.xlabel('Number of Topics')\n","_=plt.ylabel('Coherence Score')\n","plt.show()\n","\n","# Optimal model\n","lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=100, num_topics=5, workers = 4, passes=100)\n","\n","# Print topics\n","lda_model.print_topics(-1)\n","\n","# Where does a text belong to\n","lda_model[corpus][0]\n","reports['text'][0]\n","\n","# Visualize topics\n","lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n","pyLDAvis.display(lda_display)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHZzsum6tLcD","executionInfo":{"status":"aborted","timestamp":1655801242844,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["topic_values = []\n","\n","for i in range(len(reports['text'])):\n","  n = [0] * len(lda_model.print_topics(-1))\n","\n","  for v in lda_model[corpus][i]:\n","    topic = v[0]\n","    probability = v[1]\n","\n","    n[topic] = probability\n","\n","  topic_values.append(n)\n","\n","topic_values = np.array(topic_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ijq46emfut7V","executionInfo":{"status":"aborted","timestamp":1655801242844,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["topic_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hav10vvi7tyx","executionInfo":{"status":"aborted","timestamp":1655801242845,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["!pip install textstat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3g1TtIL70rf","executionInfo":{"status":"aborted","timestamp":1655801242845,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import multiprocessing\n","from tqdm import tqdm_notebook\n","import os\n","import textstat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liDrP4Wt7iU8","executionInfo":{"status":"aborted","timestamp":1655801242845,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["\n","\n","######  calculate the readability score of each row ################\n","def readability_scores_mp(data):\n","    result_dict, idx, text = data\n","\n","  #  flesch_reading_ease =  textstat.flesch_reading_ease(text)\n","    flesch_kincaid_grade =  textstat.flesch_kincaid_grade(text)\n","    dale_chall_readability_score =  textstat.dale_chall_readability_score(text) \n","    result_dict[idx] = [flesch_kincaid_grade, dale_chall_readability_score]\n","\n","def calc_readability_scores(df, col_text):\n","    manager = multiprocessing.Manager()\n","    result_dict = manager.dict()\n","    mp_list = [(result_dict, idx, title) for idx, title in enumerate(df[col_text].values)]\n","\n","    with multiprocessing.Pool(os.cpu_count()) as p:\n","        r = list(tqdm_notebook(p.imap(readability_scores_mp, mp_list), total=len(mp_list)))\n","\n","    rows = [result_dict[idx] for idx in range(df[col_text].values.shape[0])]\n","    return pd.DataFrame(rows).values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQV9PKLs73Hm","executionInfo":{"status":"aborted","timestamp":1655801242845,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["scores = np.array(calc_readability_scores(df, 'text'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pa8gxx0O8i_e","executionInfo":{"status":"aborted","timestamp":1655801242845,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["scores.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX4sZHEdNVqt","executionInfo":{"status":"aborted","timestamp":1655801242846,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from ast import literal_eval\n","df['labels'] = df['labels'].apply(literal_eval)"]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"pcDUE4BiVWsQ","executionInfo":{"status":"aborted","timestamp":1655801242846,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlNTfg8yHGA7","executionInfo":{"status":"aborted","timestamp":1655801242846,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["import gensim\n","import gensim.downloader\n","#embedding = gensim.downloader.load('glove-wiki-gigaword-300')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRrZtt1auICY","executionInfo":{"status":"aborted","timestamp":1655801242846,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["#embeddings = [ gensim.downloader.load('fasttext-wiki-news-subwords-300'), gensim.downloader.load('word2vec-google-news-300'), gensim.downloader.load('glove-wiki-gigaword-100'),  gensim.downloader.load('glove-wiki-gigaword-300'), gensim.downloader.load('glove-twitter-50'), gensim.downloader.load('glove-twitter-200') ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InF-uDHJHD-2","executionInfo":{"status":"aborted","timestamp":1655801242847,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","\n","def get_embedding_result(word, embedding):\n","  return embedding[word] if word in embedding else 1\n","\n","\n","def get_gloVe(X_train, X_test, embeddings_index):\n","  X_train_vector = []\n","  for text in X_train:\n","    words = [embeddings_index[word] for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","    #words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_train_vector.append(np.average(words, axis = 0))\n","\n","  X_test_vector = []\n","  for text in X_test:\n","    words = [embeddings_index[word] for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","    #words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    X_test_vector.append(np.average(words, axis = 0))\n","\n","    \n","  return X_train_vector, X_test_vector\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def get_gloVe_tf_idf(X_train, X_test, embeddings_index):\n","  tfidf = TfidfVectorizer()\n","  tfidf.fit(X_train)\n","  idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n","\n","  train_vector = []\n","  for text in X_train:\n","    weights = [idf_dict.get(word, 1)for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","    #weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    #words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    words = [embeddings_index[word] for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","\n","    train_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  test_vector = []\n","  for text in X_test:\n","    weights = [idf_dict.get(word,1 )for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","    #weights = [idf_dict.get(word, 1) for word in word_tokenize(text)]\n","    #words =  [get_embedding_result(word, embeddings_index) for word in word_tokenize(text)]\n","    words = [embeddings_index[word] for word in word_tokenize(text) if word in embeddings_index and word not in ultimate]\n","\n","    test_vector.append(np.average(words, axis = 0, weights = weights))\n","\n","  return train_vector, test_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oir8_oGLp8V","executionInfo":{"status":"aborted","timestamp":1655801242847,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","#### one hote enconding on the labels ########\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(df['labels'])\n","\n","#### get the text column\n","\n","X = df['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fekxL6Kzpmhz","executionInfo":{"status":"aborted","timestamp":1655801242847,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gAQcWhAbT8K","executionInfo":{"status":"aborted","timestamp":1655801242847,"user_tz":-60,"elapsed":38,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Otb-Oxy2H3Zi","executionInfo":{"status":"aborted","timestamp":1655801242848,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":39,"status":"aborted","timestamp":1655801242848,"user":{"displayName":"João Paulo","userId":"17940923821633124910"},"user_tz":-60},"id":"boE_MsIQFWNb"},"outputs":[],"source":["embedding = gensim.downloader.load('fasttext-wiki-news-subwords-300')"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","import numpy as np\n","\n","def average_sentences(X):\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      current_embedding = embed(sentences).numpy()\n","      '''\n","      diff = number -len(current_embedding)\n","      if diff > 0:\n","          padding = np.zeros((diff, 512))\n","          current_embedding = np.concatenate((padding, current_embedding), axis=0)\n","      '''\n","      new_X.append(np.mean(current_embedding, axis=0))\n","      \n","    \n","    return np.array(new_X)\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def mean_size(X, X_test):\n","\n","\n","    tfidf = TfidfVectorizer()\n","    tfidf.fit(X)\n","    idf_dict = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n","\n","    new_X = []\n","    for instance in X:\n","      sentences = sent_tokenize(instance)\n","      weights = []\n","      for sentence in sentences:\n","        weights.append(np.mean([1for word in word_tokenize(sentence)]))\n","\n","      current_embedding = embed(sentences).numpy()\n","\n","      new_X.append(np.average(current_embedding, axis=0, weights = weights))\n","\n","    X_tests = []\n","    for instance in X_test:\n","      sentences = sent_tokenize(instance)\n","      weights = []\n","      for sentence in sentences:\n","        weights.append(np.mean([1 for word in word_tokenize(sentence)]))\n","\n","      current_embedding = embed(sentences).numpy()\n","\n","      X_tests.append(np.average(current_embedding, axis=0, weights = weights))\n","      \n","    \n","    return np.array(new_X), np.array(X_tests)"],"metadata":{"id":"ds1ICxl3pBSz","executionInfo":{"status":"aborted","timestamp":1655801242848,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_y = []\n","new_X = []\n","i = 0\n","for instance in df['labels']:\n","  current_ine = []\n","  for label in instance:\n","    if label in  ['informative-support', 'emotional-support', 'esteem-support', 'network-support']:\n","      current_ine.append(label)\n","\n","  if len(current_ine) > 0:\n","    new_y.append(current_ine)\n","    new_X.append(df['text'][i])\n","  i = i + 1\n"],"metadata":{"id":"sty7atxFaO45","executionInfo":{"status":"aborted","timestamp":1655801242848,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","#### one hote enconding on the labels ########\n","mlb = MultiLabelBinarizer()\n","new_y = mlb.fit_transform(new_y)"],"metadata":{"id":"jS79e06vaxdx","executionInfo":{"status":"aborted","timestamp":1655801242849,"user_tz":-60,"elapsed":40,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlb.classes_"],"metadata":{"id":"YHmZd3WPVIEE","executionInfo":{"status":"aborted","timestamp":1655801242849,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG9hAqIrFtZF","executionInfo":{"status":"aborted","timestamp":1655801242849,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.multiclass import OneVsRestClassifier ### does not take into account label correlations\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, hamming_loss, jaccard_score\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","\n","#from sklearn.multioutput import ClassifierChain\n","\n","from sklearn.utils import compute_class_weight\n","from skmultilearn.problem_transform import BinaryRelevance\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","from skmultilearn.problem_transform import ClassifierChain, LabelPowerset\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from skmultilearn.cluster import NetworkXLabelGraphClusterer\n","from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n","from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n","\n","msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","\n","# linear is nice 0.47\n","# poly 0.37\n","# rbf 0.34\n","# sigmoid 0.43\n","\n","#from skmultilearn.ext import Keras\n","\n","from skmultilearn.adapt import MLkNN\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","################ run the pipeline #####################\n","\n","#### RF E LR\n","for train_index, test_index in msss.split(np.array(X), np.array(y)):\n","  #index = - 1\n","  #for clf in [BinaryRelevance(), ClassifierChain(), LabelPowerset()]:\n","   #index = index + 1\n","   #for tfidf in [True, False]:\n","    #for embedding_str in  [\\\n","    #   'word2vec-google-news-300'    ,\n","    #  'fasttext-wiki-news-subwords-300', \n","   #   'glove-twitter-50', #done\n","    #  'glove-twitter-200' # done\n","    #  ]:\n","\n","      #embedding = gensim.downloader.load(embedding_str)\n","    #for other_thing in [BinaryRelevance(), ClassifierChain(), LabelPowerset()]:\n","     #for tf_idf in [True, False]:\n","\n","      import warnings\n","      warnings.simplefilter(action='ignore', category=FutureWarning)\n","      ####### performing the different splits here #########\n","      X_train, X_test = X[train_index], X[test_index]\n","      y_train, y_test = y[train_index], y[test_index]\n","\n","\n","      #if tfidf:\n","      #else:\n","      #  X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","\n","\n","      from skmultilearn.problem_transform import BinaryRelevance\n","\n","\n","\n","      ### model xgboost ###~\n","      from xgboost import XGBClassifier\n","      from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","      \n","      KERAS_PARAMS = dict(epochs=15, batch_size=2, verbose=1)\n","        \n","\n","\n","      #### model b)\n","      from skmultilearn.adapt import BRkNNaClassifier # 1 parameter estimation\n","\n","      ### model c)\n","     \n","      #clf = GridSearchCV(MLkNN(), parameters, scoring=score)\n","\n","      ### model d) # alternative to for example classifier chain\n","      from sklearn.naive_bayes import GaussianNB\n","     \n","\n","      #classifier = RakelD(\n","      #    base_classifier= SVC(class_weight='balanced', random_state=42, kernel=kernels[0], C=2),\n","      #    base_classifier_require_dense=[True, True],\n","      #    labelset_size=4\n","      #)\n","\n","      ### model e) # another alternative\n","      from skmultilearn.ensemble import MajorityVotingClassifier\n","      from skmultilearn.cluster import FixedLabelSpaceClusterer\n","      from skmultilearn.problem_transform import ClassifierChain\n","      from sklearn.naive_bayes import GaussianNB\n","\n","\n","\n","      ### labels embeddings #### => nao tou a conseguir por a dar\n","      import joblib\n","      import sys\n","      sys.modules['sklearn.externals.joblib'] = joblib\n","      #from skmultilearn.embedding import CLEMS, EmbeddingClassifier\n","\n","\n","\n","      # Compile model\n","      ###### vectorize the text for training and test seperately #######\n","      from sklearn.preprocessing import FunctionTransformer\n","\n","\n","      ### perform data augmentation of the training data prior to the preprocessing phase ###\n","      #from textattack.augmentation import CheckListAugmenter\n","      #from textattack.augmentation import EasyDataAugmenter, EmbeddingAugmenter\n","\n","\n","      #X_min, y_min = get_minority_samples(X_train, pd.DataFrame(y_train, columns=mlb.classes_))\n","      #print(\"Size of the minority samples\", X_min.shape[0])\n","      number=1\n","      #checklist_augmenter = EmbeddingAugmenter(pct_words_to_swap=0.1, transformations_per_example=number)\n","\n","      #X_train, y_train = paraphrase_augmentation(X_train, y_train)\n","      \n","      #X_train, y_train = textattack_data_augment(X_train, y_train, checklist_augmenter, number)\n","\n","      #X_train, y_train = augment_data(X_train, y_train)\n","\n","      X_train = np.array(X_train)\n","      y_train = np.array(y_train)\n","\n","\n","\n","      from sklearn.neighbors import KNeighborsClassifier\n","\n","      #print(\"New dataset \", X_train.shape, \"and \", y_train.shape)\n","      from sklearn.tree import DecisionTreeClassifier\n","      #classifier = DecisionTreeClassifier()\n","      from sklearn.naive_bayes import GaussianNB\n","\n","      from sklearn.neighbors import KNeighborsClassifier\n","      from sklearn.linear_model import LogisticRegression\n","\n","      \n","      from sklearn.naive_bayes import MultinomialNB\n","      '''\n","      '''\n","      parameters = {\n","        \"vect__analyzer\": ['word'],\n","        \"vect__ngram_range\": [ (1,3)],\n","        'tfidf__use_idf': [ True,],\n","        'tfidf__norm': ['l2'],\n","        \"clf__classifier\": [SVC()],\n","        \"clf__classifier__kernel\": ['linear'],\n","        \"clf__classifier__C\": [1],\n","        \"clf__classifier__class_weight\": [\"balanced\"]\n","\n","\n","          #\"classifier\":[LogisticRegression(class_weight=\"balanced\")],\n","        #\"classifier__penalty\":['l1', 'l2'],\n","        #\"classifier__C\": [0.1, 1, 10, 100],\n","        #\"classifier__solver\": ['liblinear']\n","        #\"clf__classifier__loss\": [\"hinge\", \"squared_hinge\"],\n","        #\"clf__classifier__penalty\": ['l1', 'l2']\n","        #\"clf__classifier__var_smoothing\": [1e-8, 1e-9, 1e-10]\n","        #\"clf__n_neighbors\":[3, 5, 10],\n","        #\"clf__weights\": ['uniform', 'distance'],\n","        #\"clf__p\": [1,2 ]\n","      }\n","\n","      parameters2 = {\n","        \"classifier\": [ClassifierChain()],\n","        \"classifier__alpha\": [1e-2, 1e-3]\n","      }\n","      #X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","\n","      \n","      parameters3= {\n","        # DT\n","        #\"criterion\": ['gini', 'entropy'],\n","        #\"class_weight\": [\"balanced\"],\n","        #\"ccp_alpha\": [0, 0.005, 0.01, 0.02]\n","\n","        # \n","\n","        #\"classifier\": [GaussianNB()],\n","        #\"classifier__var_smoothing\": [1e-2, 1e-3],\n","\n","        #\"n_neighbors\":[3, 5, 10],\n","        #\"weights\": ['uniform', 'distance'],\n","        #\"p\": [1,2] ,\n","\n","\n","        #\"classifier\": [SVC()],\n","        #\"classifier__kernel\": ['linear'],\n","        #\"classifier__C\": [20],\n","        #\"classifier__class_weight\": [\"balanced\"]\n","\n","        #\"classifier\":[LogisticRegression(class_weight=\"balanced\")],\n","        #\"classifier__penalty\":['l1', 'l2'],\n","        #\"classifier__C\": [0.1, 1, 10, 100],\n","        #\"classifier__solver\": ['liblinear']\n","\n","\n","        #\"bootstrap\": [True, False],\n","        #\"max_depth\": [10, 20],\n","        #\"n_estimators\": [400, 600],\n","        #\"class_weight\":['balanced']\n","\n","\n","      \n","        #\"classifier__alpha\": [1e-2, 1e-3]\n","        #\"vect__ngram_range\": ((1, 1), (1, 2)),\n","\n","\n","        #'tfidf__use_idf': (True, False),\n","        #'tfidf__norm': ['l2', 'l1'],\n","        #\"clf__n_neighbors\":[3, 5, 10],\n","        #\"clf__weights\": ['uniform', 'distance'],\n","        #\"clf__p\": [1,2 ]\n","        #'clf__bootstrap': [True, False],\n","        #'clf__max_depth': [10, 30], #40, 50, 60, 100, None\n","        #'clf__n_estimators': [ 200, 300] # , 400, 600, 800, 1000\n","        }\n","\n","      from sklearn.metrics import make_scorer\n","\n","      #if tf_idf:\n","      #  X_train, X_test = get_gloVe_tf_idf(X_train, X_test, embedding)\n","      #else:\n","      #  X_train, X_test = get_gloVe(X_train, X_test, embedding)\n","      #X_train = vectorize_sentences(X_train)\n","      #X_test = vectorize_sentences(X_test)\n","      #X_train = np.array(X_train)\n","      #X_test = np.array(X_test)\n","\n","\n","            ###### perform feature engineering and append those features #####\n","      from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler \n","      \n","      X_train_length = get_length_features(X_train)\n","      X_test_length = get_length_features(X_test)\n","\n","      X_train_sentiment = calc_sentiment_scores(X_train)\n","      X_test_sentiment = calc_sentiment_scores(X_test)\n","\n","      X_train_read = scores[train_index]\n","      X_test_read = scores[test_index]\n","\n","      ##### topic values ########\n","\n","      X_train_topic = topic_values[train_index]\n","      X_test_topic = topic_values[test_index]\n","\n","      ######## ner part if we want #####\n","\n","      X_train_ner = append_ner_features(df2.iloc[train_index], 'tags', tags_set)[tags_set]\n","      X_test_ner = append_ner_features(df2.iloc[test_index], 'tags', tags_set)[tags_set]\n","\n","      ########### pos ##############\n","      X_train_pos = PoS_counter(X_train)\n","      X_test_pos = PoS_counter(X_test)\n","\n","\n","      feature_train = np.hstack(( \n","          X_train_pos,\n","          X_train_ner,\n","          X_train_sentiment,\n","          X_train_length,\n","                      X_train_read,\n","     \n","                      X_train_topic,\n","                   \n","                     ))\n","      \n","      normalizer = MinMaxScaler()\n","      feature_train = normalizer.fit_transform(feature_train)\n","      feature_train = sparse.csr_matrix(feature_train)\n","\n","\n","      feature_test = np.hstack(( \n","          X_test_pos,\n","          X_test_ner,\n","          X_test_sentiment,\n","          X_test_length,\n","                      X_test_read,\n","        \n","                      X_test_topic ,\n","                   \n","                   ))\n","      \n","      normalizer = MinMaxScaler()\n","      feature_test = normalizer.fit_transform(feature_test)\n","      feature_test = sparse.csr_matrix(feature_test)\n","\n","      \n","   \n","\n","\n","      from sklearn.feature_extraction.text import TfidfVectorizer\n","      \n","      #vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,3), use_idf=True, norm='l2')\n","      #X_train = vectorizer.fit_transform(X_train)\n","      #X_test = vectorizer.transform(X_test)\n","\n","      X_train  = average_sentences(X_train)\n","      X_test = average_sentences(X_test)\n","\n","\n","      \n","    \n","      X_train = sparse.hstack((\n","          sparse.csr_matrix(X_train),\n","          feature_train\n","      ))\n","      X_test = sparse.hstack((\n","          sparse.csr_matrix(X_test),\n","          feature_test\n","      ))\n","\n","      print(X_train.shape)\n","      \n","    \n","      clf = BinaryRelevance(LogisticRegression(C= 10, penalty='l2', solver='liblinear', class_weight='balanced'))\n","\n","      \n","     # text_clf = Pipeline([('vect', CountVectorizer(stop_words=ultimate)),\n","     #       ('tfidf', TfidfTransformer()),\n","            #('smote', MLSmote()),\n","            #('fs', SelectKBest(chi2, k=500)),\n","            #('dense', DenseTransformer()),\n","      #      ('clf', BinaryRelevance()),\n","      #])\n","\n","\n","      #grid_search = GridSearchCV(clf, parameters, verbose=1, cv=5)\n","      clf.fit(X_train, y_train)\n","\n","      #print(\"val\", index)\n","      print(\"Best score: %0.3f\" % grid_search.best_score_)\n","      print(\"Best parameters set:\")\n","      best_parameters = grid_search.best_estimator_.get_params()\n","      print(parameters.keys())\n","      for param_name in sorted(parameters.keys()):\n","        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n","\n","\n","      y_predict = clf.predict(X_test)\n","      # Find the best parameters for both the feature extraction and the\n","      # classifier\n","\n","      ######## APPLY SMOTED IF WANTED  ###################\n","\n","      #vectorizered_text = text_clf.fit_transform(X_train, y_train)\n","\n","      ['fasttext-wiki-news-subwords-300',\n","      'word2vec-google-news-300', #done\n","      'glove-twitter-50', #done\n","      'glove-twitter-200' # done\n","      ]\n","\n","\n","\n","      \n","  \n","      '''\n","      ### readability scores ###\n","\n","      X_train_read = scores[X_train.index]\n","      X_test_read = scores[X_test.index]\n","\n","      ##### topic values ########\n","\n","      X_train_topic = topic_values[X_train.index]\n","      X_test_topic = topic_values[X_test.index]\n","\n","      ######## ner part if we want #####\n","\n","      X_train_ner = append_ner_features(df2.iloc[X_train.index], 'tags', tags_set)[tags_set]\n","      X_test_ner = append_ner_features(df2.iloc[X_test.index], 'tags', tags_set)[tags_set]\n","\n","      print(\"ner\", X_train_ner.shape, X_test_ner.shape)\n","\n","      X_train_pos = PoS_counter(X_train)\n","      X_test_pos = PoS_counter(X_test)\n","      '''\n","      #embedding = gensim.downloader.load('glove-twitter-50')\n","\n","      #vectorizered_text, test_text = get_gloVe_tf_idf(X_train, X_test, embedding)\n","\n","      #### here for the others\n","      '''\n","      X_train = text_clf.fit_transform(X_train)\n","      test_text = text_clf.transform(X_test)\n","\n","      print(np.array(X_train).shape, np.array(test_text).shape)\n","      X_train = pd.DataFrame(data = X_train)\n","      y_train =  pd.DataFrame(y_train, columns=mlb.classes_)\n","      '''\n","\n","\n","      \n","\n","      #normalizer = MaxAbsScaler ()\n","      #X_train = normalizer.fit_transform(X_train)\n","      #X_test = normalizer.fit(X_test)\n","\n","      #features = sparse.csr_matrix(features)\n","\n","\n","      '''\n","      PERCENTAGE = 0 # 0 means that is not applied\n","      NR_NEIGHBORS = 5\n","\n","      vectorizered_text = text_clf.fit_transform(X_train, y_train)\n","      \n","      df_X = pd.DataFrame(data = vectorizered_text.toarray())\n","      df_y =  pd.DataFrame(y_train, columns=mlb.classes_)\n","\n","      X_sub, y_sub = get_minority_samples(df_X, df_y)\n","\n","      #print(\"Size of the minority samples\", X_sub.shape[0])\n","\n","      X_res, y_res = MLSMOTE(X_sub, y_sub, int(df_X.shape[0] * PERCENTAGE), NR_NEIGHBORS)\n","      '''\n","      ############## train the classifier\n","\n","      #X_train_augmented = pd.concat([df_X, X_res])\n","      #classifier.fit(pd.concat([df_X, X_res]), pd.concat([df_y, y_res]))\n","      #classifier.fit(X_train, y_train)\n","      ###### perform model validation #####################\n","\n","\n","      #y_predict = classifier.predict(X_test)\n","\n","      ### trying monte carlo ####\n","      '''\n","      test_text = text_clf.transform(X_test)\n","      mc_predictions = []\n","\n","      for i in range(20):\n","        y_p = classifier.predict(test_text)\n","        mc_predictions.append(y_p)\n","      \n","      accs = []\n","      for y_p in mc_predictions:\n","        acc = accuracy_score(y_pred=y_p, y_true=y_test)\n","        accs.append(acc)\n","\n","      print(\"MC accuracy: {:.1%}\".format(sum(accs)/len(accs)))\n","      '''\n","      #print(\"embedding\", embedding_str)\n","      #print(\"tfidf\", tfidf)\n","      from sklearn.metrics import f1_score\n","      print(\"accuracy: \", accuracy_score(y_test, y_predict))\n","      print(\"hamming loss: \", hamming_loss(y_test, y_predict))\n","      print( round(hamming_loss(y_test, y_predict),3), round(jaccard_score(y_test, y_predict, average='weighted'),3))\n","      print( \" + \".join(   [str(round(x,2)) for x in f1_score(y_test, y_predict, average=None) ]       ), \"(\", round(f1_score(y_test, y_predict, average=\"weighted\"),2), \")\")\n","      print(\"jaccard score: \", jaccard_score(y_test, y_predict, average='weighted'))\n","      print(multilabel_confusion_matrix(y_test, y_predict))\n","      print(classification_report(y_test, y_predict))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mygKh64XqlJX","executionInfo":{"status":"aborted","timestamp":1655801242849,"user_tz":-60,"elapsed":39,"user":{"displayName":"João Paulo","userId":"17940923821633124910"}}},"outputs":[],"source":["mlb.classes_"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"multilabel.ipynb","provenance":[],"authorship_tag":"ABX9TyNj34g88d6hSH9jPbTlGgx+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}